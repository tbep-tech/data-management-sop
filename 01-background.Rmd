# Background {#background}

## Importance of data

Informed decisions about how we manage behaviors and actions that affect the environment cannot be made without data.  As a fundamental part of the scientific method, data provide the raw information for conclusions on the validity of a hypothesis that informs our understanding of natural processes.  Environmental research our understanding to support informed decisions for managing natural resources and data are the foundation for research. As methods for managing environmental resources continue to evolve, so does our understanding of data beyond its conventional role in an academic setting.

When we discuss "data" we often describe a very general term that has different meanings for different people.  In a research setting, data can be the results of an experimental analysis or field survey as tabular information in it's simplest form.  For environmental managers, data can mean the long-term record of routinely collected and updated monitoring information to assess status and trends of a natural resource.  Even further, data can be highly aggregated and novel products created through complex meta-analyses of independent datasets.  In all cases, the need to describe a dataset, its purpose and origin, and to identify a permanent and long-term home for data is critical to ensure forward progress in both research and how research informs environmental management. 

This document describes one approach to address data management needs for the long-term restoration and protection of natural resources in Tampa Bay and its watershed.  We are inclusive of multiple definitions of data from simple spreadsheets to more complicated workflows for generating reporting products that support environmental decisions. This broad net is purposeful to account for both the variety of data we use as a program and the diversity of partner agencies we depend on to achieve our mission.  The management of environmental resources combines a healthy mix of conventional science with consensus driven decisions, often with strong regulatory overtones.  The use of data as the foundation for all of these processes requires a robust management plan to promote trust and validity in the environmental decision and to ensure the science continues to progress without reinventing the wheel.    

## Why we need to effectively manage data

There are many reasons why data may not be effectively managed, chief among which is that it can be tedious and unglamorous work that is often an afterthought.  The value of investing a little time in managing a dataset can have long term benefits, but these values may not be obvious or realized in the short-term.  This leads to many "orphaned" datasets and data products that at one time had a known origin and purpose, but over time this information is lost because it is not initially documented or it is not made available in a location that is findable and accessible by others.  

A classic graphic in Michener et al. [@Michener97] demonstrates how knowledge of a data product is lost over time (Figure \@ref(fig:michener)). As a research team publishes a paper, the information content of the data and metadata is at an all time high because the ideas and concepts have been intensely studied and evaluated in the months leading up to publication.  After publication, researchers move on to other projects or responsibilities and specific details about the intial project are lost rapidly as attention is focused elsewhere.  At longer time scales, other factors can contribute to the erosion of information content, including career changes through retirement or staff turnover or accidental loss of information (laptop destruction, lab fires, etc.). 

```{r michener, echo = F, fig.cap = 'Loss of information over time in the absence of data management [@Michener97]'}
knitr::include_graphics('img/michener.png')
```

The final process in Figure \@ref(fig:michener) leading to absolute and complete loss of information on a data product is death of an investigator. Although a bit morbid, this is a very real and preventable problem in the process of discovery that can you lead down the frustrating path of reconstructing a data product's origin through scouring historical records that have little to no descriptive information. As a remedy, many research teams have adopted the "bus factor" term as motivation to prevent this problem. The bus factor is an informal measurement of the relative risk associated with loss of information if an investigator was to, hypothetically, be hit by a bus (alternatively, the "lottery factor" describes departure of an individual if they were to win the lottery).  Data sets or workflows that have a high bus factor are at risk of being orphaned with the hypothetical loss of a team member.    

The costs of not effectively managing your data can vary, but each is a byproduct of neglecting an initial investment in data management at onset of a project.  In fact, you an probably recall several past instances when poor data management has come back to haunt you.  Here are a few examples, some from my own experience and some from others: 

1) A collaborator calls you on the phone asking about a historical dataset from an old report.  You spend several hours tracking down this information because you don't know where it is. The data you eventually find and provide to your collaborator has no documentation and they don't know how to use it. 
1) You receive a deliverable from a project partner that was stipulated in a scope of work.  This deliverable comes in multiple formats with no reproducible workflow to recreate the datasets.  You are unable to verify the information, eroding faith in the final product and an inability to update the results in the future.
1) An annual reporting product requires an update with new data each year.  The staff member in charge of this report spends several days gathering the new data and combining it with the historical data.  Other projects are on hold until this report is updated.  Stakeholders using this report to make decisions do not trust or misunderstan the product because the steps for its creation are opaque. 

In addition to the above examples, a more general issue is stifled creativity.  The use of other people's data and services [i.e., "OPEDAS"; @Mons18] to generate novel research or data products is increasingly common, particularly in the last twenty plus years with the advance of internet communications.  Entire disciplines and new methods have been developed around this idea (e.g., meta-analysis).  The generation of new data that have an incomplete history and metadata documentation is a disservice to both the researcher that created the data and the larger scientific community that could benefit from using this information.  As a result, scientific progress cannot progress as rapidly as it could if data products are discoverable and openly available. This leads to:

* Less collaboration in the research community
* Increased siloing among management institutions
* Less creative approaches to managing environmental resources

## Open Science 

The open science approach provides a philosophy and set of tools to help address the costs of poor data management.  Before we proceed, we need to make a distinction between the broader concept of open science and open data as one component of the former.  Many of the guidelines and examples in this SOP fall broadly under the open science umbrella (or cake as you'll see later), but it's important to understand how data management includes a set of tools that are part of, but not exhaustive of, the entire open science toolbox.  Conversely, many open science tools that can be applied to other management scenarios that are broadly applicable can also benefit data management. 

An example may be helpful.  A key component of data management that leads to more open sharing is curation of metadata.  Although metadata can be created in an open environment and is often created for the purpose of sharing, it is an entity that can be created completely in isolation with a closed workflow.  So, when we talk about metadata, the assumption is that its creation is to promote sharing and transparency for open data, whereas metadata by themselves are only so useful in that they can faciliate the latter.  

On the other hand, broader open science principles that support a culture of sharing can also have value for research workflows that have nothing to do with data.  For example, the "public school of thought" for open science focuses on making science more accessible to the general public, e.g., thorugh citizen science initiatives or science blogging.  Although this approach doesn't deal explicitly with best practices for data management, this mentality certaintly has benefit for creating a culture that appreciates and learns from science, which logically leads to discussions on the importance of data.  

For these reasons, this document covers many topics that may fall squarely under the realm of conventional data management, while in other times channeling more general open science principles with the implicit hope that can support a culture of better data management. 


* Bit rot, link rot [@Vines14]
* Professorware [@Mons18] - objects that address a novel intellectual challenge, a critical aspect of research (academic or industry) required for incremental progress, but lacking support (i.e., not scalable or sustainable).  This is problematic when these tools are embedded into larger workflows (insert post-doc bakery meme)
* Benefits of a data management workflow
* Applications in open science

## The TBEP philosophy

* How TBEP is using open science to manage internal/external data and how is this increasing transparency, reproducibility, and efficiency of our reporting products
* TBEP Open Science Subcommittee, building the community of practice

## Goals/objectives of this document

* Goal: Motivate internal staff and external partners to become stewards of their data by demonstrating the value of open data practices and providing a road map to achieving this goal.
* Link to QMP [@tbep1620] and distinction between the documents
* What it is, what it is not - including what makes TBEP different from other organizations, i.e., we have hands in lots of projects vs one central product (e.g., OHI), so our SOP needs to be generalizable
     * It is: An overview document explaining the TBEP approach to data management, explains philosophy in details, tools we have developed
     * It is: A cookbook describing how to manage datasets in an open science framework, including considerations before, during, and after a project, with full realization that data management should begin during project planning but often does not
     * It is not: A definitive overview of best practices for data management
     * It is not: A definitive overview of available online services for opening data, but we lean towards use of specific platforms that we find useful
* Intended audience: TBEP internal staff and external partners, targeted towards technical staff in the latter case while also appealing to managers/admin that can create space to foster better practices for data management, also see TBEP QMP sec 1.3 
* Document structure
