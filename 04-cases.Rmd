# Case Studies {#cases}

```{r}
library(dplyr)
load(file = url('https://github.com/tbep-tech/tberf-oyster/raw/main/data/oysdat.RData'))
load(file = url('https://github.com/tbep-tech/tberf-oyster/raw/main/data/sitdat.RData'))
load(file = url('https://github.com/tbep-tech/tberf-oyster/raw/main/data/wqmdat.RData'))
```

In this section we describe three case studies to demonstrate how data management workflows are developed in the wild.  In section \@ref(automation), we presented a comprehensive workflow for how we developed our water quality report card.  The examples in this section are similar by adopting elements of the previously described workflow, but with some important differences. The examples here represent data products resulting from TBEP and partner-funded research as opposed to a reporting product and, more importantly, all of the data management workflows for these projects were developed after the projects were started.  This is a no-no for data management, but we provide these examples to demonstrate how we've applied the principles in this document to inopportune but realistic situations. Each example describes the general goals and questions of the project, then outlines the thought process to identifying and documenting important data products.

## Oyster restoration in Tampa Bay {#oyster}

```{r, out.width = '50%', fig.cap = 'Restoration of oyster reefs (*Crassostrea virginica*) is a critical management goal to support key habitats in Tampa Bay.'}
knitr::include_graphics('img/oysters.jpg')
```

Establishment and restoration of oyster reefs in Tampa Bay is a critical programmatic goal defined under our [Comprehensive Conservation and Management Plan](https://indd.adobe.com/view/cf7b3c48-d2b2-4713-921c-c2a0d4466632) [@tbep1017] and [Habitat Master Plan Update](https://drive.google.com/file/d/1Hp0l_qtbxp1JxKJoGatdyuANSzQrpL0I/view) [@tbep0720].  Oyster reefs are formed by the cumulative build up of shell material over time and provide food and habitat, reduce erosion, stabilize shorelines, and improve water quality.  Recreational and commercial harvest of oysters can also provide economic support for the region.  The historical distribution of oyster populations in Tampa Bay is poorly documented, although anecdotal evidence suggests current coverage of oysters Tampa Bay is far less than previously occurred.  Establishment and restoration of oyster reefs have been fundamental activities to re-establish viable oyster populations in the Bay.  

Critical questions on factors that contribute to the successful establishment or restoration of oysters in Tampa Bay need to be answered to achieve our programmatic goals.  This includes the collection of data to describe the long-term success of natural and restored sites, including location in the Bay, seasonal timing of restoration activities, and preferred restoration materials for varying conditions.  In addition, standardized monitoring protocols for restoration sites to evaluate or estimate long-term success are needed.  This project involves establishing restoration sites at different locations and collecting field data to address relevant questions. 

At the time of writing, field data have been collected for the first year of the project and is stored in multiple spreadsheets in an Excel workbook (figure \@ref(fig:oysexc)).  The field data is a likely candidate for the most important data contribution of this project and a plan for curating these data has recently been developed.  This plan is primarily focused on answering questions to identify which factors promote long-term success of oyster reefs, with the intent of formatting the data for analysis to answer these questions and delivering the data in a way to reproduce the results.  Environmental managers (e.g., partners that conduct restoration) may have interest in the results (i.e., analysis outcomes), whereas additional researchers may have an interest in using the raw data to support follow-up analysis or to synthesize the information with other datasets.   

```{r oysexc, fig.cap = "A screenshot of the raw oyster data from the first year of field work.  The data are close to tidy, but information is spread across tables with no easy way to link between them.", out.width = '100%'}
knitr::include_graphics('img/oysterexcel.PNG')
```

The current approach for managing these data have focused on adopting a tidy format for the existing information.  Because field data collection has already begun, we developed a post-hoc workflow to wrangle the information into flat files with appropriate keys to link data between tables.  Identifying a permanent home for these data and formal documentation of metadata have not been done, although tidying the data will aid analysis and facilitate documentation and delivery of final data products when the time is right (e.g., sooner rather than later).  In this example, we focus on the steps to tidy the data. 

Our tidying workflow for the first year of field data is available in a GitHub repository: https://github.com/tbep-tech/tberf-oyster. The raw data are available in a the `data/raw` folder and processing to make them "tidy" is accomplished through a custom analysis script at `R/dat_proc.R`.  The analysis script converts the raw data present in multiple sheets in the Excel workbook to three separate tidy tables.  We use functions from the tidyr and dplyr R packages as part of the tidyverse [@Wickham19] to format relevant information from the raw data to create the separate tidy tables.  This process also involved discussion with project partners when ambiguous labels were observed in data or presented as conflicting information between tables.   

The final "tidy" tables include three flat files for the site data (table \@ref(tab:tabsitdat)), water quality data at each site (table \@ref(tab:tabwqmdat)), and oyster data at each site (table \@ref(tab:taboysdat)).  

```{r tabsitdat}
knitr::kable(head(sitdat), caption = 'First six rows of the site data.', label = 'tabsitdat')
```

```{r tabwqmdta}
knitr::kable(head(wqmdat), caption = 'First six rows ofthe water quality data.', label = 'tabwqmdat')
```

```{r taboysdat}
knitr::kable(head(oysdat), caption = 'First six rows of the oyster data.', label = 'taboysdat')
```

Each table is in a tidy format with 1) each variable having its own column, 2) each observation in its own cell, and 3) each value having its own cell.  The only exception to these rules is the `id` column which is a combination of site name, restoration type (bags, domes, shell, etc.), installation year, and installation season.  Technically, this column violates the third rule of tidy data by including multiple values (site name, type, etc.) in one cell.  However, the creation of the `id` column was purposeful to achieve two goals.  First, we wanted to create a unique identifier for each restoration site based on our questions of site location, type, and time of installation because all of these characteristics can be used to evaluate the key research questions for the project.  Identifying a site only by its name (e.g., 2D Island only) would not have allowed a comparison of installation years, for example, at the same site, thus it was important to include all identifying characteristics in the `id` to facilitate the analysis.  Second, we wanted the unique identifier to easily convey key information about each site.  We could have used a random text string for each unique combination of site, type, installation year, and installation season, but it would be close to impossible to determine key information about each site without viewing table \@ref(tabsitdat).

The `id` keys also allow us to easily join tables for follow-up analysis.  For example, we can easily join the oyster data and water quality data for downstream analysis using some R functions from the tidyverse:

```{r, echo = T}
combdat <- full_join(oysdat, wqmdat, by = 'id')
head(combdat)
```

Storing the data in these three tidy tables reduces redundant information, organizes the data by general categories (e.g., oysters vs water quality), and facilitates follow-up analysis.  The GitHub repository also includes an [exploratory analysis](https://tbep-tech.github.io/tberf-oyster/figures.html) of these data created with RMarkdown [@Xie18] to combine code and text in an HTML format.  This web page is also [built automatically](https://github.com/tbep-tech/tberf-oyster/actions) with GitHub Actions each time the source document is updated (see section \@ref(automation)). 

In this example, its useful to understand reasons why raw data are often structured in an untidy format.  Raw data from field or experimental observations are often setup for ease of entry, whereas tidy data are setup for ease of analysis. Entering data in the field in a tidy format or by hand from field sheets when you're back in the office may seem unnatural.  Conceptualizing core components of each dataset and the links between them that can facilitate downstream analyses can also be challenging at early stages of a research project.  Data wrangling will always be a necessary component of data management, but working towards manual entry in as tidy a form as possible will reduce time on the backend when preparing the data for analysis or delivery at the end of a project.

## DeSoto/RESTORE project {#desoto}

* Initial questions - how can we fulfill RESTORE requirements for data delivery based on a general grant requirement?
* Example of continuous stream data
* Emphasis on CI/CD checks and web products

## Red Tide Twitter repo {#twitter}

* Initial questions - What are the most relevant products from this project and how can we make them accessible?
* Example of specific data product with linkage to technical and primary lit publication
* Emphasis on creating a GitHub repo for archive of lexicon and source data, DOI
