<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 3 Key Concepts and Principles | Data Management SOP for the Tampa Bay Estuary Program</title>
  <meta name="description" content="This is a web page describing the TBEP data management SOP" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 3 Key Concepts and Principles | Data Management SOP for the Tampa Bay Estuary Program" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a web page describing the TBEP data management SOP" />
  <meta name="github-repo" content="tbep-tech/data-management-sop" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 3 Key Concepts and Principles | Data Management SOP for the Tampa Bay Estuary Program" />
  
  <meta name="twitter:description" content="This is a web page describing the TBEP data management SOP" />
  

<meta name="author" content="Marcus W. Beck, Gary E. Raulerson, Maya C. Burke, Joe Whalen, Sheila Scolaro, Ed T. Sherwood" />


<meta name="date" content="2021-02-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="background.html"/>
<link rel="next" href="workflow.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Management SOP</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#contrib"><i class="fa fa-check"></i><b>1.1</b> Contributing to this document</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#opt1"><i class="fa fa-check"></i><b>1.1.1</b> Option 1</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#opt2"><i class="fa fa-check"></i><b>1.1.2</b> Option 2</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#opt3"><i class="fa fa-check"></i><b>1.1.3</b> Option 3</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#opt4"><i class="fa fa-check"></i><b>1.1.4</b> Option 4</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#about"><i class="fa fa-check"></i><b>1.2</b> About</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>2</b> Background</a><ul>
<li class="chapter" data-level="2.1" data-path="background.html"><a href="background.html#dataimp"><i class="fa fa-check"></i><b>2.1</b> Importance of data</a></li>
<li class="chapter" data-level="2.2" data-path="background.html"><a href="background.html#why-we-need-to-effectively-manage-data"><i class="fa fa-check"></i><b>2.2</b> Why we need to effectively manage data</a></li>
<li class="chapter" data-level="2.3" data-path="background.html"><a href="background.html#open-science"><i class="fa fa-check"></i><b>2.3</b> Open Science</a></li>
<li class="chapter" data-level="2.4" data-path="background.html"><a href="background.html#the-tbep-philosophy"><i class="fa fa-check"></i><b>2.4</b> The TBEP philosophy</a></li>
<li class="chapter" data-level="2.5" data-path="background.html"><a href="background.html#goals-and-objectives-of-this-document"><i class="fa fa-check"></i><b>2.5</b> Goals and objectives of this document</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="keys.html"><a href="keys.html"><i class="fa fa-check"></i><b>3</b> Key Concepts and Principles</a><ul>
<li class="chapter" data-level="3.1" data-path="keys.html"><a href="keys.html#general-concepts"><i class="fa fa-check"></i><b>3.1</b> General concepts</a><ul>
<li class="chapter" data-level="3.1.1" data-path="keys.html"><a href="keys.html#data-types-and-identifying-important-contributions"><i class="fa fa-check"></i><b>3.1.1</b> Data types and identifying important contributions</a></li>
<li class="chapter" data-level="3.1.2" data-path="keys.html"><a href="keys.html#the-fair-principles"><i class="fa fa-check"></i><b>3.1.2</b> The FAIR principles</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="keys.html"><a href="keys.html#specific-concepts"><i class="fa fa-check"></i><b>3.2</b> Specific concepts</a><ul>
<li class="chapter" data-level="3.2.1" data-path="keys.html"><a href="keys.html#metadata"><i class="fa fa-check"></i><b>3.2.1</b> Metadata</a></li>
<li class="chapter" data-level="3.2.2" data-path="keys.html"><a href="keys.html#where-do-data-live"><i class="fa fa-check"></i><b>3.2.2</b> Where do data live?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>4</b> Data Management Workflow</a><ul>
<li class="chapter" data-level="4.1" data-path="workflow.html"><a href="workflow.html#the-tbep-approach"><i class="fa fa-check"></i><b>4.1</b> The TBEP approach</a><ul>
<li class="chapter" data-level="4.1.1" data-path="workflow.html"><a href="workflow.html#philo"><i class="fa fa-check"></i><b>4.1.1</b> Our philosophy</a></li>
<li class="chapter" data-level="4.1.2" data-path="workflow.html"><a href="workflow.html#tools"><i class="fa fa-check"></i><b>4.1.2</b> Tools we use</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="workflow.html"><a href="workflow.html#howyou"><i class="fa fa-check"></i><b>4.2</b> How can you manage data?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="workflow.html"><a href="workflow.html#im-at-the-beginning-of-my-project"><i class="fa fa-check"></i><b>4.2.1</b> I’m at the beginning of my project</a></li>
<li class="chapter" data-level="4.2.2" data-path="workflow.html"><a href="workflow.html#im-somewhere-in-the-middle-of-my-project"><i class="fa fa-check"></i><b>4.2.2</b> I’m somewhere in the middle of my project</a></li>
<li class="chapter" data-level="4.2.3" data-path="workflow.html"><a href="workflow.html#im-at-the-end-of-my-project"><i class="fa fa-check"></i><b>4.2.3</b> I’m at the end of my project</a></li>
<li class="chapter" data-level="4.2.4" data-path="workflow.html"><a href="workflow.html#metadata-workflow"><i class="fa fa-check"></i><b>4.2.4</b> Metadata workflow</a></li>
<li class="chapter" data-level="4.2.5" data-path="workflow.html"><a href="workflow.html#lets-get-it-online"><i class="fa fa-check"></i><b>4.2.5</b> Let’s get it online!</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cases.html"><a href="cases.html"><i class="fa fa-check"></i><b>5</b> Case Studies</a><ul>
<li class="chapter" data-level="5.1" data-path="cases.html"><a href="cases.html#tberf-oyster-restoration-project"><i class="fa fa-check"></i><b>5.1</b> TBERF oyster restoration project</a></li>
<li class="chapter" data-level="5.2" data-path="cases.html"><a href="cases.html#desoto"><i class="fa fa-check"></i><b>5.2</b> DeSoto/RESTORE project</a></li>
<li class="chapter" data-level="5.3" data-path="cases.html"><a href="cases.html#red-tide-twitter-repo"><i class="fa fa-check"></i><b>5.3</b> Red Tide Twitter repo</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final.html"><a href="final.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="7" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>7</b> Appendices</a><ul>
<li class="chapter" data-level="7.1" data-path="appendices.html"><a href="appendices.html#list-of-resources"><i class="fa fa-check"></i><b>7.1</b> List of resources</a></li>
<li class="chapter" data-level="7.2" data-path="appendices.html"><a href="appendices.html#data-types"><i class="fa fa-check"></i><b>7.2</b> Data types</a></li>
<li class="chapter" data-level="7.3" data-path="appendices.html"><a href="appendices.html#definitions"><i class="fa fa-check"></i><b>7.3</b> Definitions</a></li>
<li class="chapter" data-level="7.4" data-path="appendices.html"><a href="appendices.html#metadata-templates"><i class="fa fa-check"></i><b>7.4</b> Metadata templates</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Management SOP for the Tampa Bay Estuary Program</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="keys" class="section level1">
<h1><span class="header-section-number">Section 3</span> Key Concepts and Principles</h1>
<p>Before we get started, we need to discuss some basic ideas around data and their management. Understanding these concepts and why they’re important will facilitate the development and curation of open data for both you and others to use. We start with general concepts and then dive into some detailed concepts. The detailed concepts may seem daunting, but they are critical in supporting your journey in managing your own data.</p>
<div id="general-concepts" class="section level2">
<h2><span class="header-section-number">3.1</span> General concepts</h2>
<div id="data-types-and-identifying-important-contributions" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Data types and identifying important contributions</h3>
<p>We briefly introduced a general concept of data in section <a href="background.html#dataimp">2.1</a>. Throughout this document, we use the term “data” to describe a variety of products either directly supporting decision-making processes or used for research to support the former. Data can be generated to support or refute hypotheses in research, whereas research can also produce data products that support environmental management. The end game in all of these processes is understanding that data can be present at any stage in research and/or decisions that support environmental management. Individuals may generally use the term “data” to describe products at any point in this workflow. Understanding the diferent ways we talk about data will allow you to more carefully identify your data management needs.</p>
<p>Identifying the types of data that are important to support decision-making is the first task in developing a data management workflow. Any research project could produce countless datasets and it may be challenging to understand which datasets are important or are merely intermediate steps in a larger process. To help you identify which datasets are important to your project, ask these questions:</p>
<ol style="list-style-type: decimal">
<li>What is the most important and tangible contribution of this project?<br />
</li>
<li>Who is going to benefit from the results of this project?</li>
<li>How can I use data management practices to make the use of these data “easier” for decision-making?</li>
</ol>
<p>Answers to these questions can help you identify important data products that need formal data management workflows. However, coming to a single answer is the exception, not the norm, and a typical answer usually is “it depends”. Also realize that you may be the direct beneficiary of a particular research project - documenting and using proper data management workflows will save you from headaches in the future. Evaluating these questions at different steps throughout a project can help you identify the valuable contributions.</p>
<p>In a perfect world where we have endless time and resources, and not to mention interest, to dedicate to data management, we would track and document the provenance of every single dataset used by a research project. Of course, this is impractical, nor do we need to curate every piece of data. You will need to identify the most important contribution of a project among alternatives based on your answers to the above questions. Here are a couple scenarios that can help in this process.</p>
<blockquote>
<p>I am collecting field data and/or running experiments in a laboratory.</p>
</blockquote>
<p>The field or experimental data are obvious candidates for developing a data management workflow, yet it is rarely a solitary dataset that is produced. Working with these data continuously throughout a project will benefit from developing a data dictionary and understanding linking keys between different data tables. If you don’t want or need to archive all the datasets you’ve used or created, identify a master dataset that provides the main results for your study.</p>
<blockquote>
<p>I am using data from an external source as primary or secondary information to support analysis or generate a reporting product</p>
</blockquote>
<p>A derived dataset may be the most important contribution of this project. This dataset includes multiple combinations of input datasets from external sources. It is important to document the steps that were used to develop this dataset, including the raw sources of information and where they can be accessed. Documentation can range from a general description of the dataset (less desirable) to complete access to source code for reproducing the derived dataset (more desirable). The most important contribution may be the workflow or the derived dataset, depending on “who” can benefit most from this project.</p>
<blockquote>
<p>I am producing a model to support scenario exploration or understanding of natural processes</p>
</blockquote>
<p>Tracking data provenance of a modelling project is a challenging task simply because a “model” does not conform to the conventional understanding of data. As noted above, we describe data as anything that can support decision-making in environmental management. Models are commonly used for this task, yet understanding of their information content over time often rests with one individual, giving that modeller a very high bus factor. There are practical limitations for fully tracking a model as a data product (e.g., computational limits, time requirements, required knowledge of its working components), but there are certainly derived datasets from models that can benefit from data management. In particular, model results, parameters, or source code are all prime candidates for data management, depending on the audience.</p>
<blockquote>
<p>I am developing a decision-support tool</p>
</blockquote>
<p>Related to the challenges of data management for modelling, so-called “decision-support” tools are increasingly used as a front-end for decision-makers to access relevant information from a research project or intensive data collection effort. Online interactive dashboards have proliferated tremendously in the last ten years to meet this need. These tools can be useful in the right hands, yet there is no community standard for how to treat these products as data to track their origin and metadata. In this case, documenting the workflow, source code, and requisite datasets for powering the dashboard may be the most important contributions.</p>
</div>
<div id="the-fair-principles" class="section level3">
<h3><span class="header-section-number">3.1.2</span> The FAIR principles</h3>
<p>The previous section presented several questions to ask yourself that can aid in identifying important contributions of a research project as a focus for data management. In all cases, once that important contribution is identified, community standards or best practices for that dataset or product should be used to ensure the intended audience can find, access, use, and replicate the data. The FAIR principles <span class="citation">(Wilkinson et al. <a href="#ref-Wilkinson16" role="doc-biblioref">2016</a>)</span> provide some general guidelines to follow for ensuring the openness of a data product. The FAIR acronym is described as follows:</p>
<ul>
<li><strong>F</strong>indable: The data have a globally unique and persistent identifier, including use of “rich” metadata.</li>
<li><strong>A</strong>ccessible: Once found, the data can be retrieved using standardized communications protocols that are open, free, and universally implementable.</li>
<li><strong>I</strong>interoperable: The ability of data or tools from non-cooperating resources to integrate or work together with minimal effort.</li>
<li><strong>R</strong>eusable: If the above are achieved, the data and metadata are described in a way that they can be replicated and/or combined in different settings.</li>
</ul>
<p>What this means simply is that 1) each dataset has a name that doesn’t change and can be found with minimal effort using that name, 2) once it’s found, you can actually get your hands on it (e.g., not behind a paywall), 3) once you have it, you can use readily available tools to work with the data (e.g., not using proprietary software), and 4) you can actually apply the data for your own needs because it has sufficient context, including its reproduction, given the the first three principles are met.</p>
<p>In practice, the FAIR principles invoke several concepts that will be described in detail later, but we describe some here as a gentle introduction. The term “globally unique and persistent identifier” (under <strong>F</strong>) is a mouthful that simply means the dataset has a name assigned to itself that is not assigned to any other dataset (globally unique) and it’s permanent (persistent). This doesn’t mean a descriptive or literal name, such as you would assign to a file on your own computer, rather it means a computer-generated identifier created using a known standard. One such example is a <a href="https://www.doi.org/">DOI</a>, or digital object identifier. These are commonly assigned to publications as a static web address (unique and persistent) and are increasingly being used as identifiers for datasets.</p>
<p>Findable and accessible also imply the data have a home with an address. The latter describes the unique identifier, whereas the home itself is permanent location as a requirement for accessibility. There are several options for where data can live long-term and theoretically forever so long as the internet exists. There are literally thousands of repositories online that can be used for data archival and the answer to which repository you should use is almost always going to be “it depends”. We provide some examples in section <a href="workflow.html#tools">4.1.2</a> as one option used by TBEP.</p>
<p>The FAIR principles are not rigorous standards, rather they establish general questions you should ask of a dataset to make sure you’ve done your due diligence in achieving openness. Further, because they are not rigorously defined, different organizations may interpret the principles differently and it’s important to realize that your understanding of the principles may differ from others. For example, individuals may define “reusable” in different ways that can affect the level of detail provided in the metadata. These principles are presented here as a reminder to think about them often, especially during the beginning of a project, and how they can be applied in opening the most important contribution of your project.</p>
</div>
</div>
<div id="specific-concepts" class="section level2">
<h2><span class="header-section-number">3.2</span> Specific concepts</h2>
<ul>
<li>Types of data products (e.g., raw data, models, synthesized/derived data, etc.) or types of data (flat file, spatial, disparate)</li>
<li>Tabular data
<ul>
<li>an overview of tidy data, can a machine read it?</li>
<li>The wrong approach</li>
</ul></li>
<li>Basic database principles
<ul>
<li>logical extension of tidy data</li>
<li>normalized tables (including discussion of key variables), what are unique ids (e.g., tberf oyster, how did I make the unique id?), facilitate standard DB queries</li>
</ul></li>
</ul>
<div id="metadata" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Metadata</h3>
<p>Just as “data” can have different meanings to different people, “metadata” is a loosely defined term that describes one of the most important aspects of data management. Metadata varies from simple text descriptions of a dataset, such as “who”, “what”, “when”, “where”, “why”, and “how”, to more formalized standards with the intent of preparing your data for archival in a long-term repository. Having no metadata is almost a guarantee that your dataset will be orphaned or misused by others, either inadvertently or with willful acknowledgment that the original purpose of the data is unknown and its use may be inappropriate for the task at hand. Metadata are also important for enabling discovery of your data (the <strong>F</strong> in FAIR). So, when you think of data management, you should think of it as synonymous with metadata curation.</p>
<p>At its basic level, metadata is literally defined as “data about data” or “information about information”. A more comprehensive definition is provided by <span class="citation">Gilliland (<a href="#ref-Gilliland16" role="doc-biblioref">2016</a>)</span>:</p>
<blockquote>
<p>A suite of industry or disciplinary standards as well as additional internal and external documentation and other data necessary for the identification, representation, interoperability, technical management, performance, and use of data contained in an information system</p>
</blockquote>
<p>We use this definition as a starting point to develop our thinking around best practices for metadata generation and curation. Again, it’s good to emphasize that some metadata is way better than no metadata at all. Just because you are not using industry or disciplinary standards for generating metadata doesn’t mean you’re approach is incorrect. As you get comfortable with the general purpose of metadata and how it’s developed as a description for a dataset, you can build on this knowledge by adopting more formalized standards for developing metadata.</p>
<p>At its basic level, think of metadata as a simple text file containing the information about your dataset. This text file provides answers to common questions about the origin of your data so that anyone (or a computer) with zero knowledge about your data can quickly orient themselves as to what the data represents and its purpose. The US Geological Survey provides a useful document on creating <a href="https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/Metadata%20in%20Plain%20Language_508compliant.pdf">Metadata in “plain language”</a> to distill the basic information contained in a metadata file. As indicated above, it provides a workflow for answering the “who”, “what”, “when”, “where”, “why”, and “how” questions for metadata. We provide a brief synopsis of these questions below. You can use this workflow to generate your own metadata.</p>
<blockquote>
<p>What does the dataset describe?</p>
</blockquote>
<p>Information here would include very basic details about the dataset including a <strong>title</strong>, <strong>geographic extent</strong>, and <strong>period of time</strong> covered by the data. For geographic extent, this may often include explicit coordinates covering the study area, i.e., the lower left and upper right of a bounding box. Location is useful for indexing your dataset relative to others, if for example, a researcher wanted to find data for all studies in the geographic extent of Tampa Bay. Other useful information about the “what” might include the type of data, e.g., tabular, map, online dashboard, etc.</p>
<blockquote>
<p>Who produced the dataset?</p>
</blockquote>
<p>This would be yourself and anyone else who has made a significant contribution to the development of a dataset. People may have differing opinions regarding what defines a “significant” contribution, but as the curator of a dataset, it’s up to you to determine how important it is for including an individual as a contributor. Data are increasingly being used as citable resources and including individuals that were important in its generation ensures proper attribution. For scientific publications, each author is generally expected to have made substantial contributions to the study conception and design, data acquisition or analysis, or interpretation of results. The same would apply to data. If someone has spent hours toiling in the field to collect the data or hours visually scanning a spreadsheet for quality control, include them!</p>
<blockquote>
<p>Why was the dataset created?</p>
</blockquote>
<p>Describing why a dataset was created is critically important for developing context. If others want to use your data, they need to know if its appropriate for their needs. Here you would describe the goal or objectives of the research for which the data were collected. It should be clear if there are limitations in your data defined by your goals. For example, you may have collecte field data in a particular time of year to address questions about seasonal changes. Using these data to answer broader temporal questions, such as inter-annual changes, would not be inappropriate and could lead to wrong conclusions if someone using your data were not aware of this limitation. Identifying the “why” of your dataset could also prevent misinterpretation or misuse of the data by non-specialists. Think of it as an insurance policy for your data.</p>
<blockquote>
<p>How was the dataset created?</p>
</blockquote>
<p>Here you would describe the methods used to generate the data, e.g., field sampling techniques, laboratory methods, etc. This information is important so others can know if you’ve used proper and accepted methods for generating the data. Citing existing SOPs or methods that are recognized standards in your field would be appropriate. If you are generating a synthesis data product using data from external sources, make sure to document where those data come from and the methods you used for synthesis. Pay attention to documenting the software that was used, including the version numbers. If you have analysis code or script that was used for synthesis, provide a link if possible.</p>
<blockquote>
<p>How reliable are the data?</p>
</blockquote>
<p>It’s also very important to document aspects of a dataset that affect reliability. The answers you provide to the above questions can provide context to this reliability, but it’s also imporant to explicitly note instances when the data could be questionable or inappropriate to use. Here you could describe any quality assurance or quality control (QAQC) checks that were used on the data. There are often formalized ways to do so, such as codes or descriptors in tabular data defining QAQC values (e.g., data in range, below detection, sensor out of service, etc.). You will want to clearly describe what each of these codes mean and if they cover the range of conditions possible for your data. Other QAQC procedures, such as how the data were verified for accuracy, can also be described.</p>
<blockquote>
<p>How can someone get a copy of the dataset?</p>
</blockquote>
<p>Good metadata always has information on who has the data and how to contact them for requesting access. For archived or publicly available data, this information is more important for who to contact should someone have questions. Information on obtaining a copy of the data should also describe any special software or licensing issues related to accessing the data. Under the <strong>I</strong> in FAIR, you should strive to make your data as interoperable as possible and not store your data in an obscure format that requires specialized software. If this is unavoidable (e.g., your data are large and it needs to be compressed), describe what needs to be done to access the data. Any licensing or permissions issues on using data should also be described, e.g., is it free for use with or without attribution, are there limitations on its use, etc. The <a href="https://r-pkgs.org/license.html">licensing chapter</a> in <span class="citation">Wickham and Bryan (<a href="#ref-Wickham15" role="doc-biblioref">2015</a>)</span> is a great place to start to learn more about licensing. Although this chapter relates to code licensing, the same principles could apply to data.</p>
<div id="metadata-examples" class="section level4">
<h4><span class="header-section-number">3.2.1.1</span> Metadata examples</h4>
<p>Now that we’ve covered the general concepts of what is included in metadata, we provide some examples of what this looks like in practice. At it’s simplest, metadata can simply be a text file that includes information on the questions above. Below is an example of a simple text file that accompanies a dataset that we describe in section <a href="cases.html#desoto">5.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:desotometa"></span>
<img src="img/desotometa.PNG" alt="A simple example of metadata illustring the principle that something is better than nothing."  />
<p class="caption">
Figure 3.1: A simple example of metadata illustring the principle that something is better than nothing.
</p>
</div>
<p>Just by looking at the metadata, we can quickly understand what this dataset provides. This is a simple text file that accompanies some water quality monitoring data at two buoys near Ft. DeSota in Pinellas Co, Florida. We can see the type of data, how often it’s collected, what equipment was used, the location of the buoys, some contact information should there be questions, and other items that provide context. Although it doesn’t cover all of the questions above, I would be more than happy to use this data since I have some basic knowledge about what’s included.</p>
<p>The example in figure <a href="keys.html#fig:desotometa">3.1</a> represents the bare minimum of what should be done to document metadata. This metadata is an excellent example of the principle that some metadata is better than no metadata. So many datasets lack even the simplest information to facilitate their use by others. At its core, metadata should serve the purpose of providing information about information. No matter the level of specificity or metadata standard that was used, all metadata serve this purpose. However, formalized approaches to documenting metadata can also serve an important purpose of preparding a dataset for discovery and long-term archiving. The next section provides one example of a metadata standard that could be used for environmental datasets.</p>
</div>
<div id="the-eml-standard" class="section level4">
<h4><span class="header-section-number">3.2.1.2</span> The EML standard</h4>
<p>There are countless standards for metadata that go beyond the simple descriptive text shown above. These standards provide a formalized approach or “schema” to documenting metadata that provides context about a dataset that is also machine readable. The latter component is critical for making sure that all datasets prepared for hosting or archiving at a particular location follow the same standards for documenting metadata. The core pieces of information (who, what, when, where, why, and how) are included, but in formalized way to allow for rapid searching and queries when the data are stored at a centralized location with hundreds to thousands of other datasets.</p>
<p>One such standard that is useful for environmental data is the <a href="https://eml.ecoinformatics.org/">Ecological Metadata Language</a> or EML. The EML standard defines a comprehensive vocabulary and a readable XML markup syntax (fancy talk for machine readable) for documenting research data. Importantly, the standard is community maintained and developed for environmental researchers who want to openly share their data. The EML standard is also used by the <a href="https://knb.ecoinformatics.org/">Knowledge Network for Biocomplexity</a> or KNB, which is an online repository that is federated with a much larger network of online data repositories.</p>
<p>The EML metadata file is an XML file that looks something like this:</p>
<div class="figure" style="text-align: center"><span id="fig:emlsimple"></span>
<img src="img/emlsimple.PNG" alt="A very simple example of an EML file for metadata, shown as an XML file."  />
<p class="caption">
Figure 3.2: A very simple example of an EML file for metadata, shown as an XML file.
</p>
</div>
<p>The file in figure <a href="keys.html#fig:emlsimple">3.2</a> might look complicated, but it’s just a way to document the basic components of metadata so that a machine can read them. Regarding the descriptive role of metadata, the above example provides a title for the dataset, a brief description, and who to contact. All the rest is additional information about the standard that was used and basic XML tags to close the document. The EML provides standards to document any and all other types of metadata information for the questions described above.</p>
<p>A specific reason why EML is mentioned here is the availability of additional software tools to help create EML files for your data. In particular, the <a href="https://docs.ropensci.org/EML/">EML</a> R package provides these tools to streamline the creation of metadata. Nobody wants to type an XML file by hand, so<br />
the EML packages provides a set of functions where a user can input basic metadata information to create the XML file automatically. All you need is a basic understanding of R and metadata to use the EML package for your own metadata. More information can be found on the website: <a href="https://docs.ropensci.org/EML/" class="uri">https://docs.ropensci.org/EML/</a></p>
<p>Of course, you can always manually enter your metadata when you submit a dataset to an online repository. Most repositories, KNB included, provide a form entry system for doing so. This may not be the most efficient choice, but is often the preferred for first-timers that may not yet be comfortable with using other tools to generate metadata.</p>
</div>
<div id="data-dictionaries" class="section level4">
<h4><span class="header-section-number">3.2.1.3</span> Data dictionaries</h4>
<p>A final note about metadata relates to data dictionaries and what they mean for describing a dataset. A data dictionary can be used for tabular datasets to describe column names and the type of data in each column. This can be incredibly useful for understanding context of a dataset, which is why we include this in the metadata section. However, data dictionaries also have importance for more general best practices for data management. Simple things like how you name a data column can have larger implications for downstream analysis pipelines or interpretability for a dataset. In metadata, a data dictionary can be as simple as the example in figure <a href="keys.html#fig:desotometa">3.1</a> for the parameters that were collected. There we see the column names, units, and formats for time variables. This is invaluable information for any data analyst.</p>
<p>Here we provide some general guidelines for developing your own data dictionary. This is all information that can be included in metadata, but it is also useful to consider for data management.</p>
<blockquote>
<p>Column names</p>
</blockquote>
<p>Be as descriptive as possible while trying to keep the name as short as possible. Really long names with lots of detail can be just as frustrating as very short names with very little detail. Ideally, the description of data in a column can be included in metadata, but the column name should also point the analyst in the right direction. Try to avoid spaces in column names since some software may interpret that as the start of a new column.</p>
<blockquote>
<p>Column types</p>
</blockquote>
<p>Each column includes only one type of data, e.g., temperature measurements, categorical descriptors, or counts of observations. Never, ever mix data types in the same column. If your data are continuous numeric values, try to identify an acceptable range for the values, e.g., are there minimum or maximum values that would indicate the data are out of range? Also make note of the units that were used. For categorical descriptors, identify all possible categories that are acceptable, e.g., small, medium, or large for a qualitative descriptor of size. For dates, make note of the format, e.g., YYYY-MM-DD. For time, identify the timezone.</p>
</div>
</div>
<div id="where-do-data-live" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Where do data live?</h3>
<ul>
<li>Where do data live long-term, what’s a doi, considering a data paper, federated repository, etc.
<ul>
<li>The A in FAIR</li>
<li>GitHub repository</li>
<li>Stable URL</li>
<li>Official repository</li>
</ul></li>
</ul>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Gilliland16">
<p>Gilliland, A. J. 2016. “Setting the Stage.” In <em>Introduction to Metadata</em>, 3rd ed. Los Angeles, California: Getty Publications.</p>
</div>
<div id="ref-Wickham15">
<p>Wickham, H., and J. Bryan. 2015. <em>R Packages</em>. Sebastopol, California: O’Reilly.</p>
</div>
<div id="ref-Wilkinson16">
<p>Wilkinson, M. D., M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Axton, A. Baak, N. Blomberg, et al. 2016. “The Fair Guiding Principles for Scientific Data Management and Stewardship.” <em>Scientific Data</em> 3 (160018). <a href="https://doi.org/10.1038/sdata.2016.18">https://doi.org/10.1038/sdata.2016.18</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="background.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="workflow.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tbep-tech/data-management-sop/edit/main/02-keys.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["data-management-sop.pdf", "data-management-sop.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
