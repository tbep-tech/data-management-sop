# Final Words {#final}

This data management SOP is our best attempt at describing a general philosophy of how the TBEP approaches data management and a general framework for how others could manage data at their own institutions.  We included a discussion of general and specific topics that are useful to understand for data management (section \@ref(keys)), a description of our philosophy towards data management (section \@ref(philo)), a generic workflow for managing data (section \@ref(howyou)), and some case case studies demonstrating how these principles play out in the real word (section \@ref(cases)).  Our approach is constantly evolving as we works towards a more cohesive plan for managing data and the tools described in this SOP will form the foundation of our approach as we figure out what works and what doesn't work for our organization and our partners.  

We finish this document by describing some general themes and lessons learned that should guide your approach towards data management. Whether you choose to use the specific tools we mention here (e.g., GitHub, R, Shiny, etc.) or adopt other techniques, the themes and lessons present throughout this document still apply.  We reiterate them here as a reminder to approach data management with these principles in mind. 

## Something is better than nothing

Novice data stewards can be overwhelmed by the apparent need to "check all the boxes" in the open science workflow of data management.  This might include full metadata documentation using an accepted standard like EML, full version control of data workflows on GitHub, linking a repository with archive services like Zenodo, developing comprehensive data dictionaries, formatting all data in tidy format, and mastering open source data science languages like R.  This can be especially daunting when considering that multiple data products could be considered "valuable contributions" of a research project.  

Unless you have a fully dedicated IT support team and all the time in the world, it's impractical to try to adopt all of the principles in this document and apply them to every single piece of data for a project.  Even applying all of these principles to the single most important data contribution of a project can be impractical. In light of this challenge, the tendency may be to simply treat data in a familiar way using entrenched workflows where data is seen only as a commodity that serves to address the research question at hand. 

We absolutely encourage you not to fall back on old habits and consider the fact that __something is better than nothing__ when it comes to data management.  Perhaps you set a goal of only checking one box for a particular project.  Maybe you start by developing a simple metadata text file or developing a data dictionary. Even if you accomplish only one data management related task, this is a vast improvement over doing nothing at all.  Channeling this concept, @Wilson17 discuss "good enough practices" in scientific computing acknowledging the fact that very few of use are professionally trained in data science.  So, be kind to yourself when learning new skills and realize that the first step will likely be frustration, but through frustration comes experience.  The more comfortable you are in a new task, the more likely you'll apply it to other data products. You're efficiency will increase over time, making you more likely to attempt additional data management tasks in the future.  

> “Dude, suckin’ at something is the first step to being sorta good at something." - Jake The Dog, [Adventure Time](https://en.wikipedia.org/wiki/Adventure_Time)

## Just remember FAIR

We presented the FAIR principles early on in section \@ref(fair) as a set of guiding concepts that could be applied to any data management plan. Invoking these principles when managing data can help establish a set of "goal posts" to strive to achieve for any data product.  If you have questions about whether or not your plan for managing a data product is appropriate, go through each of the FAIR principles to see they align with your plans.  If not, consider an alternative approach or what you can modify in your plan to make them satisfy these principles.

When applying the FAIR principles, there are two considerations to keep in mind.  First, we previously mentioned that the principles are purposefully vague as they describe only a general approach to achieving openness.  As a result, the principles can have different interpretations to different people.  What one data stewards considers "findable" may not be considered "findable" by another data steward.  This challenge absolutely applies to our descriptions of the tools we described in this SOP.  For example, we heavily rely on GitHub in our data management workflows and suggest that data may be FAIR if stored on GitHub.  Others may strongly disagree with this approach because GitHub was developed primarily as a code management platform and not a long-term archive for data storage. This reflects a difference of opinion on what is findable, accessible, interoperable, and reusable. 

That being said, the second consideration in applying the FAIR principles is that they also exist on a spectrum and you should not reasonably expect to check all of the boxes to make your data product is completely open when first developing a data management plan.  You choose what each of the letters mean in FAIR based on your needs for data management or the needs of your organization.  Over time, you'll more easily be able to address each of the components of FAIR, but they should be considered guiding principles rather than something that can be rigorously defined.     

## The ever-evolving toolbox

The ability of a larger community of developers to contribute to the development of open source software, such as R, is what makes it so great.  The existing tools are visible to others and can be built upon to fix bugs or add enhancements. A much more robust and flexible product is created than proprietary software that is only exposed to a small cabal of developers.  However, this benefit is two-sided in that the tools are constantly changing.  As tools change, analysis code that once worked may behave differently or not at all.  

Any data scientist will admit that a key challenge to maintaining a relevant skillset is staying abreast of the constantly evolving toolbox in the open source community.  If you choose to incorporate open source software into your data management workflows, consider the potential burden of having to maintain that software as the community contributes to the source code or other packages that your software may depend on.  This is not an impossible task, but does require a bit of attention on your part to make sure your code is up to date and plays well with the current toolbox available to the wider community.  For example, monitoring the [#RStats](https://twitter.com/hashtag/rstats) hashtag on Twitter can be a good way to monitor the "conversation" around existing toolsets.  Many of the lead developers actively tweet to announce changes or to solicit input on what could be done to improve software.  Tracking issues on GitHub for specific packages can also be a good approach to see which changes are taking place or which software packages are actively used by others.  

It's also entirely possible that broadly used tools like R or Python may no longer be relevant in the not too distant future.  The historical evolution of software makes this inevitable.  
* evolving tools
* trying to be both a domain expert and data expert will spread you thin [@Mons18, p. 27, 36], look to the helpers/community (Figure \@ref(fig:codehero)) 
* Join the community of practice by getting involved in the [Open Science Subcommittee](https://tbep.org/our-work/boards-committees/technical-advisory-committee/#open-science)
* Next step, metadata entry forms as administered through TBERF, custom APIs?  

```{r codehero, out.width = '100%', fig.cap = 'Look to the helpers and your open science community! Artwork by [\\@allison_horst](https://twitter.com/allison_horst).'}
knitr::include_graphics('img/code_hero.jpg')
```

2ref
