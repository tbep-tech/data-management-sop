# Data Management Workflow {#workflow}

This section is in two parts to first describe a workflow that we use internally at TBEP to manage our data and then to describe a road map for opening internal or external datasets at your own organization.  The first section expands on our philosophy for using open science to manage data, including specific workflows we use, as context to the second section.  Our approach is one way of applying open science to managing data.  Applying the exact same approach at your organization may or may not be appropriate depending on your internal and external needs for managing data.  As such, our approach is generalizable and modular - any of the approaches can be modified in part or together for your own needs. 

## The TBEP approach

### Our philosophy {#philo}

Sections \@ref(opengeneral) and \@ref(philogeneral) introduced you to our basic philosophy and approach to managing data at TBEP.  As an organization that facilitates science, management, and outreach activities among our local partners, we adopt open science as a cornerstore strategy that will serve the Program's core values.  This approach is made explicit in our [Strategic Plan](https://drive.google.com/file/d/11xohuoaHDxNHRqgXoOHdI37FpWvac_rn/view?usp=sharing) that describes how we achieve programmatic goals defined under our [Comprehensive conservation and Management Plan](https://indd.adobe.com/view/cf7b3c48-d2b2-4713-921c-c2a0d4466632) (CCMP) and who we can work with in our [Interlocal Agreement](https://drive.google.com/file/d/1iJcWxmc5SeyDTqiCQ3MLQGWEY_EDGtZT/view?usp=sharing) to help us achieve our goals. 

Our data [Quality Management Plan](https://drive.google.com/file/d/1DyA0PNHV8rEXGMwGiyS7sXY1ECLYpJJO/view) (QMP, @Sherwood20) is a companion document to this SOP that ensures the data used by TBEP for decision-making has known and documented quality and is being used appropriately. The QMP establishes an internal process for verifying data quality standards that conform with federal requirements we have as an organization funded in part by federal dollars under Section 320 of the [Clean Water Act](https://www.epa.gov/laws-regulations/summary-clean-water-act).  On the other hand, this SOP is a more hands-on and accessible document that describes a how-to approach for data management that we adopt as an organization.  The SOP goes beyond the QMP by exposing the process and ideas behind how we manage data at TBEP so that others can learn from our experience.  We encourage you to view our QMP to understand the literal benchmark we use an organization to ensure quality of our data.

We actively work to apply open science every activity we pursue to achieve our goals under the CCMP.  

* The Open Science cake: what it is, how we do we implement it, and what does it mean for data management, figure \@ref(fig:cake)

```{r cake, fig.cap = 'The open science cake showing the connection between research, environmental decisions, and the public.'}
knitr::include_graphics('img/cake.png')
```

* Applied science, not implied science
* General workflow - source to product, figure \@ref(fig:osworkflow)

```{r osworkflow, fig.cap = 'The TBEP open science workflow connecting source data to decision-support products.'}
knitr::include_graphics('img/os-workflow.png')
```

### Tools we use {#tools}

* R/RStudio IDE workflow
     * The tbeptools package as central component
     * How does the package facilitate the above?
* GitHub as a collaborative tool and quasi-archive
     * version control
     * collaborative tool to work together - issues, pull requests
     * DOI through Zenodo
     * CI/CD, Automation with GitHub Actions and badges
     * GitHub linkage to TBEP website

## How can you manage data? {#howyou}

Ten simple rules for creating a good data management plan: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004525

* Section is written as a road map for developing a data product, there will be steps/checkboxes/forms, roughly following figure \@ref(fig:dataworkflow)

```{r dataworkflow, fig.cap = 'A hypothetical and generalized timeline for managing data associated with a project.', out.width = '80%'}
knitr::include_graphics('img/dataworkflow.png')
```

* Modularity is key to reproducibility, it is independent of where you're at in the project
* Setup some kind of flow chart (if this, then that)

### I'm at the beginning of my project

* What type of project am I working on? 
* What types of products am I expecting?
* Which datasets are important? Guidance for determining which datasets are important are expressed in great detail in section \@ref(keys).
* How do I want to make the data accessible?
* What QA protocols should be established?

### I'm somewhere in the middle of my project

* Have I collected data?
* Are my data in tidy format (if tabular)? 
* Have I been documenting metadata? 

### I'm at the end of my project

* Time for damage control

### Metadata workflow

* USGS resources <https://www.usgs.gov/products/data-and-tools/data-management>
* Metadata questionnaire <https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/atoms/files/MetadataQuestionnaire_508compliant.pdf>
* Data dictionaries <https://www.usgs.gov/products/data-and-tools/data-management/data-dictionaries>

### Let's get it online!

* How to of where do you put your data
