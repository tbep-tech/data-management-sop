[["index.html", "Data Management SOP for the Tampa Bay Estuary Program Section 1 Overview 1.1 Contributing to this document 1.2 About", " Data Management SOP for the Tampa Bay Estuary Program Marcus W. Beck, Gary E. Raulerson, Maya C. Burke, Joe Whalen, Sheila Scolaro, Ed T. Sherwood 2021-02-17 Section 1 Overview Welcome to the Tampa Bay Estuary Program Data Management SOP. This document describes our philosophy for managing data used by our Program and serves as motivation for our external partners to become stewards of their own data. Working together, we can improve how data are curated and used to support the continued protection and restoration of Tampa Bay. 1.1 Contributing to this document Using an open science ethos, we strongly encourage community collaboration in how this document evolves. This means anybody can contribute directly to content in this document. Please follow the guidelines in section 1.1 to learn how to contribute and improve this SOP. This SOP was created using bookdown, which is an approach to creating long-form documents with RMarkdown. The source code is available on the TBEP GitHub group web page: https://github.com/tbep-tech/data-management-sop. Each section is a plain .Rmd text file that can be edited or commented to provide feedback on content. There are several ways you can contribute to or edit this document. Before you choose your editing option, you should be comfortable with Git/GitHub basics and have some working knowledge of RMarkdown files (but see 1.1.4). The first step is to make sure you have a GitHub account so you can edit the files. Jenny Bryan’s Happpy Git and GitHub for the useR is an excellent resource to get started with version control. R Markdown: The Definitive Guide is a great resource for learning RMarkdown (also see the cheatsheet). 1.1.1 Option 1 Requires: GitHub account, write access to the source code repository Each section can be edited directly by selecting the edit button at the top of the page. Clicking on the edit button will take you to GitHub, where you will see an edit page like this: Each edit page is specific to the section where you’ve selected the edit button, e.g., if you click the edit button for section 2, you’ll be sent to the edit page for the .Rmd file for section 2. Feel free to make any changes on the .Rmd file. When you’re done, scroll to the bottom and “commit” your changes. This simply means you write a few words describing the edits you’ve made. Be as succinct as possible. When you’re done, hit the green “Commit changes” button. 1.1.2 Option 2 Requires: Github account Follow the above steps in 1.1.1 by navigating to a section you’d like to edit on this website and selecting the edit button. If you don’t have write access to the repository, you will see something like this: This simply means that you need to create your own copy to edit. You can fork your own copy to your personal account and make your edits there. Once editing is done, you can submit a pull request to the original repository with your proposed changes. Not sure what this means? Check out this chapter here: https://happygitwithr.com/fork-and-clone.html 1.1.3 Option 3 Requires: GitHub account If none of the above sounds appealing, you can always post any suggestions or edits as an issue under the issues tab of the repository. When you create a new issue by clicking the giant green “New issue” button, you’ll see something like this: Give your issue a short but informative title (e.g., “suggests edits to section 2”). Under the “Write” tab, explain what edits or changes you’d like to see. Feel free to select a member of the TBEP staff to assign the issue using the menu on the right. The issues descriptions support Markdown syntax, so get creative in your descriptions (i.e., make lists, link to documents, etc., see the cheatsheet). In general, one issue should cover only one suggested change to the document. However, multiple text edits to the same document can be submitted to the same issue so long as they cover similar topics, e.g., one issue for several suggested edits to one section. 1.1.4 Option 4 Requires: Email Just email me any changes you’d like to see! 1.2 About The Tampa Bay Estuary Program is one of 28 National Estuary Programs designated by Congress to restore and protect “estuaries of national significance.” Administered by the U.S Environmental Protection Agency under the Clean Water Act, each program must develop a science-based plan using community input to protect and enhance the natural resources of its respective estuary and surrounding watershed. The Comprehensive Conservation and Management Plan (CCMP, updated in 2017) presents 39 actions to sustain progress in bay restoration through the year 2027. To address the actions in our CCMP, our 2021-2025 Strategic Plan provides a framework to guide decisions about how to align personnel and financial resources with the Program’s mission in ways that maximize our impact on Tampa Bay recovery. A cornerstone strategy of this plan is the use of open science principles and methods to allow the TBEP to be the primary source of trusted, unbiased, and actionable science for the Tampa Bay Estuary. This document is a reflection of these strategies. Please visit our website for additional information about our program: https://www.tbep.org This book is licensed under a Creative Commons Attribution 4.0 International License. This version of the book was built automatically with GitHub Actions on 2021-02-17. "],["background.html", "Section 2 Background 2.1 Importance of data 2.2 Why we need to effectively manage data 2.3 Open Science 2.4 The TBEP philosophy 2.5 Goals and objectives of this document", " Section 2 Background 2.1 Importance of data Data is critical to making informed policies and decisions about how we manage behaviors and actions that affect the environment. As a fundamental part of the scientific method, data provide the raw information to support hypotheses that inform our understanding of natural processes. Data are the foundation for environmental research which develops this understanding to support informed decisions for managing natural resources. As methods for managing environmental resources continue to evolve, so does our understanding of data and its potential applications. When we discuss “data” we often describe a very general term that has different meanings for different people. In it’s simplest form, data can be tabular information for the results of an experimental analysis or field survey. For environmental managers, data can mean the long-term record of routinely collected and updated monitoring information to assess status and trends of a natural resource. Even further, data can be highly aggregated and novel products created through complex meta-analyses of independent datasets. In all cases, the need to describe a dataset’s purpose and origin and identify its permanent and long-term home are critical to ensure forward progress in both conventional research and how research informs environmental management. This document describes one approach to address data management needs for the long-term restoration and protection of natural resources in Tampa Bay and its watershed. We are inclusive of multiple definitions of data from simple spreadsheets to more complicated workflows for generating reporting products that support environmental decisions. This broad net is purposeful to account for both the variety of data we use as a Program and the diversity of partner agencies we depend on to achieve our mission. The management of environmental resources combines a healthy mix of conventional science with consensus driven decisions, often with strong regulatory overtones. The foundation for all of these processes requires a robust approach for working with data that promotes trust and validity in the environmental decision and to ensure the science continues to progress without reinventing the wheel. 2.2 Why we need to effectively manage data There are many reasons why data may not be effectively managed, chief among which is that it can be tedious and unglamorous work that is often an afterthought. The value of investing time upfront to managing a dataset can have long term benefits, however these values may not be obvious or realized in the short-term. Lack of effective data management results in “orphaned” datasets and data products which at one point had a known origin and purpose, but over time this information is lost because formal documentation and accessiblility by others is lacking. A classic graphic in Michener et al. (Michener et al. 1997) demonstrates how knowledge of a data product is lost over time (Figure 2.1). As a research team publishes a paper, the information content of the data and metadata is at an all time high because the ideas and concepts have been intensely studied and evaluated in the months leading up to publication. After publication, researchers move on to other projects or responsibilities and specific details about the intial project are lost rapidly as attention is focused elsewhere. At longer time scales, other factors can contribute to the erosion of information content, including career changes through retirement or staff turnover or accidental loss of information (laptop takes a bath, lab fires, etc.). Figure 2.1: Loss of information over time in the absence of data management (Michener et al. 1997) The final step in Figure 2.1 leading to absolute and complete loss of information on a data product is death of an investigator. Although a bit morbid, this is a very real and preventable problem in the process of discovery that can you lead down the frustrating path of reconstructing a data product’s origin by scouring historical records that have little to no descriptive information. As a remedy, many research teams have adopted the “bus factor” term as motivation to prevent this problem. The bus factor is an informal measurement of the relative risk associated with loss of information if an investigator was to, hypothetically, be hit by a bus (alternatively, the “lottery factor” describes departure of an individual if they were to win the lottery). Datasets or workflows that have a high bus factor are at high risk of being orphaned with the departure of a team member. The costs of not effectively managing your data can vary, but each is a byproduct of neglecting an investment in data management. In fact, you can probably recall several past instances when poor data management has come back to haunt you. Here are a few examples, some from my own experience and some from others: A collaborator calls you on the phone asking about a historical dataset from an old report. You spend several hours tracking down this information because you don’t know where it is. The data you eventually find and provide to your collaborator has no documentation and they don’t know how to use it or use it inappropriately. You receive a deliverable from a project partner that was stipulated in a scope of work. This deliverable comes in multiple formats with no reproducible workflow to recreate the datasets. You are unable to verify the information, eroding your faith in the final product and making it impossible to update the results in the future. An annual reporting product requires using new data each year. The staff member in charge of this report spends several days gathering the new data and combining it with the historical data. Other projects are on hold until this report is updated. Stakeholders using this report to make decisions do not trust or misunderstand the product because the steps for its creation are opaque. A more general problem with poor data management is stifled creativity. The use of other people’s data and services (i.e., “OPEDAS”; Mons 2018) to generate novel research or data products is increasingly common, particularly in the last twenty plus years with the advance of internet communications. Entire disciplines and new methods have been developed around this idea (e.g., meta-analysis; Carpenter et al. 2009; Lortie 2014). The generation of new data that have an incomplete history or that lack metadata documentation is a disservice to both the researcher that created the data and the larger scientific community that could benefit from using this information. As a result, scientific progress will not continue as rapidly as it could if data products are discoverable and openly available. Poor data management can also lead to peculiar or entrenched workflows that are not scalable or translatable for other users. Many of us, myself included, have our own preferences for how we manage our data that simply “works for us”, either because we learned out of necessity because the work had to be done or we’ve just been doing it a certain way for so long that it now seems normal despite being inefficient or prone to error. In extreme cases, this can lead to workflows that may seem legitimate but are problematic because they lack a formality or standards that are common in other disciplines. Mons (2018) describes “professorware” as one type of workflow for handling or generating data that address a novel intellectual challenge, which are important in research or discovery, but are not scalable or sustainable in the long run. Think of a pet project where you’ve written some code to achieve a certain task. It might be clunky, but you’re proud of it because it gets the job done on your computer and saves you from having to do a task by hand. These workflows often masquerade as novel “software packages” that do great things, which they can and often do, but they lack support because they’re not often developed using community standards or best practices for long-term use or scalability. This is especially problematic when these workflows are intentionally or unintentionally embedded into larger data management systems. If one piece of the system lacks provenance or support, it puts the larger data management system at risk. In summary, poor data management practices can lead to the following: Less collaboration in the research community Increased siloing among management institutions Less creative approaches to managing environmental resources Inefficient and error prone workflows that are neither scalable nor sustainable 2.3 Open Science The open science approach provides a philosophy and set of tools to help address the costs of poor data management. Before we proceed, we need to make a distinction between the broader concept of open science and open data as one component of the former. Many of the guidelines and examples in this SOP fall broadly under the open science umbrella (or cake as you’ll see later), but it’s important to understand how data management includes a set of tools that are part of, but not exhaustive of, the entire open science toolbox. Conversely, many broadly applicable open science tools that can be applied to other management scenarios can also benefit data management. An example may be helpful. A key component of data management that leads to more open sharing is metadata. Although metadata can be created in an open environment and is often created for the purpose of facilitating openness, it can also be created completely in isolation with a closed workflow. So, when we talk about metadata, the assumption is that its creation is to promote sharing and transparency for open data, whereas metadata by themselves are only so useful in how they can facilitate the latter. On the other hand, broader open science principles that support a culture of sharing can also have value for research workflows that generally have nothing to do with data. For example, the “public school of thought” for open science focuses on making science more accessible to the general public, e.g., through citizen science initiatives or science blogging (Fecher and Friesike 2014). Although this approach doesn’t deal explicitly with best practices for data management, this mentality certainly has benefit for creating a culture that appreciates and learns from science, which logically leads to discussions on the importance of data. For these reasons, this document covers many topics that may fall squarely under the realm of data management, while at other times advocating for more general open science principles with the intent of supporting a culture of better data management. 2.4 The TBEP philosophy The Tampa Bay Estuary Program (TBEP) is one of 28 National Estuary Programs designated by Congress to restore and protect “estuaries of national significance.” Many of these estuaries are heavily urban (i.e., having economic, recreational, cultural importance) and have had historical or ongoing issues contributing to poor environmental quality. The recovery of Tampa Bay is an exceptional story of an urban estuary that demonstrates the value of the NEP approach to restoring and protecting environmental resources. Through a coordinated regional effort of environmental professionals, utility operators, community members, and local politicians, nutrient loads to the Bay have been reduced by ~2⁄3 from 1970s levels and seagrasses have surpassed the 1950s benchmark extent (Greening et al. 2014; Sherwood et al. 2017). Even more remarkable is that while the human population in the Tampa Bay watershed continues to increase, nutrient loads into the Bay remain low. The TBEP is a key facilitator among the many local partners that have an interest in the region’s natural resources. Our facilitation is guided by several documents, including an Interlocal Agreement with our partners, a Comprehensive Conservation and Management Plan, and a Strategic Plan. In simple terms, these documents respectively describe who we work with, what we need to accomplish, and how it can be accomplished. Open science and data management have everything to do with how we facilitate Bay management. Our recent update to the Strategic Plan specifically speaks to our use of open science as 1) a general direction for how we accomplish our work to achieve the desired future state of Tampa Bay and 2) as a unique value proposition that TBEP offers within its sphere of influence. We articulate the use of open science at TBEP as a cornerstone strategy: Be the primary source of trusted, unbiased, and actionable science for the Tampa Bay estuary, recognizing that open science principles will serve the Program’s core values. As a program with only seven employees, we realize our success depends on the work of our many partners. While we use open science principles internally, we can have a much greater impact if our partners understand the value of open science and actively work towards adopting its principles in their own workflows. We are actively supporting our partners through this journey through an Open Science Subcommittee that has a goal of developing a community of practice that works and learns together to navigate the open science landscape. Our roles and responsibilities document explains how we are accomplishing this goal. Our program rests on a strong foundation of research that guides decision-making for Tampa Bay covering three decades of science and collaboration. Although great strides have been made, we strive to do better as an organization, starting with enhanced data management practices guided by open science principles. The details of this approach and why we’ve adopted open science as our own are explained fully in section 4.1.1. 2.5 Goals and objectives of this document The overarching goal of this document is to achieve the following: Motivate internal staff and external partners to become stewards of their data by demonstrating the value of open data practices and providing a road map to achieving this goal. Each section of the SOP address a critical topic or provides a roadmap that collectively helps us work towards achieving this goal. The sections are as follows: Section 3: An overview of general and specific topics that are useful to understand for data management Section 4: An explanation of how TBEP manages data and a roadmap to developing your own workflows Section 5: Examples describing specific projects and why/how data management practices were applied to each. Section 6: Parting thoughts and words of wisdom to help you continue on your open science and data management journey Section 7: A list of resources for continue learning or additional content supporting earlier sections The TBEP has also developed a data Quality Management Plan (QMP; E.T. Sherwood, G. Raulerson, M. Beck, M. Burke 2020). This SOP and the QMP can be viewed as partner documents that are complementary to each other, but are developed to meet different needs. The goal of this SOP is described above. The goal of the QMP is to ensure the data used by TBEP for decision-making has known and documented quality and is being used appropriately. In other words, the QMP establishes an internal process for ensuring data quality standards are in conformance with federal requirements, whereas the SOP document is a generalized introduction and how-to approach for data management that can help us achieve goals of the QMP. Identifying what this SOP is and what it is not can also help us set expectations for what this document can achieve. As noted above, the TBEP is a relatively small organization with hands in many projects supported or managed primarily by our partners. It would be inappropriate and impossible to describe a detailed step-by-step SOP that could apply to every project. So, the approach and workflows we describe are meant to be generalizable to many types of projects. Any specificity that is described relates to how to use tools that have broad applicability, e.g., developing a GitHub workflow or describing general characteristics of metadata that could apply to many data types. This distinguishes our SOP from others that may apply rigorous standards to one particular problem. To summarize, this document is: An explanation of the TBEP approach to data management, including our philosophy and the existing tools we have developed A generalized cookbook describing how to manage datasets in an open science framework, including considerations before, during, and after a project This document is not: A definitive overview of best practices for data management, there are other resources (see section 7) that cover these topics in more detail A comprehensive list of available online services for opening data, although we certainly lean towards specific platforms that we find useful Finally, the intended audience for this SOP is TBEP internal staff and our external partners. In both cases, the text is written to target technical staff, although the concepts and principles that are advocated should also appeal to managers or higher administrative staff. These individuals are in a position to foster better practices for data management by creating space and time for technical staff to adopt these new workflows. Understanding the importance of the tools is important, but sufficient space must be available for these skillsets to grow through a shared community of practice. Over time, the return on investment in creating a space for these skillsets to develop will be realized. References "],["keys.html", "Section 3 Key Concepts and Principles 3.1 Identifying important contributions 3.2 The FAIR principles 3.3 The importance of tidy data 3.4 Metadata 3.5 Where do data live?", " Section 3 Key Concepts and Principles Before we get started, we need to discuss some basic ideas around data and their management. Understanding these concepts and why they’re important will facilitate the development and curation of open data for both you and others to use. Some of these concepts are very general, whereas others may seem fairly specific. The detailed concepts may seem daunting, but they are critical in supporting your journey in managing your own data. 3.1 Identifying important contributions We briefly introduced a general concept of data in section 2.1. Throughout this document, we use the term “data” to describe a variety of products either directly supporting decision-making processes or used for research to support the former. Data can be generated to support or refute hypotheses in research, whereas research can also produce data products that support environmental management. The end game in all of these processes is understanding that data can be present at any stage in research and/or decisions that support environmental management. Individuals may generally use the term “data” to describe products at any point in this workflow. Understanding the diferent ways we talk about data will allow you to more carefully identify your data management needs. Identifying the types of data that are important to support decision-making is the first task in developing a data management workflow. Any research project could produce countless datasets and it may be challenging to understand which datasets are important or are merely intermediate steps in a larger process. To help you identify which datasets are important to your project, ask these questions: What is the most important and tangible contribution of this project? Who is going to benefit from the results of this project? How can I use data management practices to make the use of these data “easier” for decision-making? Answers to these questions can help you identify important data products that need formal data management workflows. However, coming to a single answer is the exception, not the norm, and a typical answer usually is “it depends”. Also realize that you may be the direct beneficiary of a particular research project - documenting and using proper data management workflows will save you from headaches in the future. Evaluating these questions at different steps throughout a project can help you identify the valuable contributions. In a perfect world where we have endless time and resources, and not to mention interest, to dedicate to data management, we would track and document the provenance of every single dataset used by a research project. Of course, this is impractical and we do not need to curate every piece of data. You will need to identify the most important contribution of a project among alternatives based on your answers to the above questions. Here are a couple scenarios that can help in this process. I am collecting field data and/or running experiments in a laboratory. The field or experimental data are obvious candidates for developing a data management workflow, yet it is rarely a solitary dataset that is produced. Working with these data continuously throughout a project will benefit from developing a data dictionary (section 3.4.3) and understanding linking keys between different data tables. If you don’t want or need to archive all the datasets you’ve used or created, identify a master dataset that provides the main results for your study. I am using data from an external source as primary or secondary information to support analysis or generate a reporting product A derived dataset may be the most important contribution of this project. This dataset includes multiple combinations of input datasets from external sources. It is important to document the steps that were used to develop this dataset, including the raw sources of information and where they can be accessed. Documentation can range from a general description of the dataset (less desirable) to complete access to source code for reproducing the derived dataset (more desirable). The most important contribution may be the workflow or the derived dataset, depending on “who” can benefit most from this project. I am producing a model to support scenario exploration or understanding of natural processes Tracking data provenance of a modelling project is a challenging task simply because a “model” does not conform to the conventional understanding of data. As noted above, we describe data as anything that can support decision-making in environmental management. Models are commonly used for this task, yet understanding of their information content over time often rests with one individual, giving that modeller a very high bus factor. There are practical limitations for fully tracking a model as a data product (e.g., computational limits, time requirements, required knowledge of its working components), but there are certainly derived datasets from models that can benefit from data management. In particular, model results, parameters, or source code are all prime candidates for data management, depending on the audience. I am developing a decision-support tool Related to the challenges of data management for modelling, so-called “decision-support” tools are increasingly used as a front-end for decision-makers to access relevant information from a research project or intensive data collection effort. Online interactive dashboards have proliferated tremendously in the last ten years to meet this need. These tools can be useful in the right hands, yet there is no community standard for how to treat these products as data to track their origin and metadata. In this case, documenting the workflow, source code, and requisite datasets for powering the dashboard may be the most important contributions. In summary, identifying the most important data contribution is a challenge that can guided through careful evaluation of the above. This may lead you to choose one or more data products to develop a data management workflow for a specific project. These could include: Tabular data either as standalone or as several tables linked by common keys Derived or synthesis data, often tabular, created as the sum of other, disparate datasets Model output or model information that describe environmental processes or likely outcomes of management scenarios Workflows to creating a data product, which could include analysis code as a continuous pipeline from source to product An online dashboard to support user engagement with data 3.2 The FAIR principles The previous section presented several questions to ask yourself that can aid in identifying important contributions of a research project as a focus for data management. In all cases, once that important contribution is identified, community standards or best practices for that dataset or product should be used to ensure the intended audience can find, access, use, and replicate the data. The FAIR principles (Wilkinson et al. 2016) provide some general guidelines to follow for ensuring the openness of a data product. The FAIR acronym is described as follows: Findable: The data have a globally unique and persistent identifier, including use of “rich” metadata. Accessible: Once found, the data can be retrieved using standardized communications protocols that are open, free, and universally implementable. Iinteroperable: The ability of data or tools from non-cooperating resources to integrate or work together with minimal effort. Reusable: If the above are achieved, the data and metadata are described in a way that they can be replicated and/or combined in different settings. What this means simply is that 1) each dataset has a name that doesn’t change and can be found with minimal effort using that name, 2) once it’s found, you can actually get your hands on it (e.g., not behind a paywall), 3) once you have it, you can use readily available tools to work with the data (e.g., not using proprietary software), and 4) you can actually apply the data for your own needs because it has sufficient context, including its reproduction, given the the first three principles are met. In practice, the FAIR principles invoke several concepts that will be described in detail later, but we describe some here as a gentle introduction. The term “globally unique and persistent identifier” (under F) is a mouthful that simply means the dataset has a name assigned to itself that is not assigned to any other dataset (globally unique) and it’s permanent (persistent). This doesn’t mean a descriptive or literal name, such as you would assign to a file on your own computer, rather it means a computer-generated identifier created using a known standard. One such example is a DOI, or digital object identifier. These are commonly assigned to publications as a static web address (unique and persistent) and are increasingly being used as identifiers for datasets. Findable and accessible also imply the data have a home with an address. The latter describes the unique identifier, whereas the home itself is permanent location as a requirement for accessibility. There are several options for where data can live long-term and theoretically forever so long as the internet exists. There are literally thousands of repositories online that can be used for data archival and the answer to which repository you should use is almost always going to be “it depends”. We provide some examples in section 4.1.2 as one option used by TBEP. The FAIR principles are not rigorous standards, rather they establish general questions you should ask of a dataset to make sure you’ve done your due diligence in achieving openness. Further, because they are not rigorously defined, different organizations may interpret the principles differently and it’s important to realize that your understanding of the principles may differ from others. For example, individuals may define “reusable” in different ways that can affect the level of detail provided in the metadata. These principles are presented here as a reminder to think about them often, especially during the beginning of a project, and how they can be applied in opening the most important contribution of your project. 3.3 The importance of tidy data So far we have covered a variety of data products ranging from tabular data to more abstract definitions that may include analysis pipelines or online services. Tabular data are by far the most recognized and most common data type and it’s worth covering a few basic principles for managing these data that will help you tremendously in the long run. At their core, tabular data are a simple conceptual model for storing information as observations in rows and variables in columns, yet its very common to try to make a table more than it should be. Unless you spend a lot of time working with data, it can be difficult to recognize common mistakes that lead to table abuse. Before we get into tidy data, I want to rant a bit about Excel. It may seem elitist, but my intentions are pure. There are many examples that demonstrate how Excel has contributed to the abuse of tables and even to the detriment of science (???). Although it is a very interesting and clever program, it is not software developed for data storage. It is a graphical user interface masquerading as database software. It includes many tools that may appear useful for organizing information, but that ultimately increase risk and make your life as an analyst more difficult. Excel allows you to abuse your data in many ways, such as adding color to cells, embedding formulas, and automatically formatting cell types. The problem occurs when this organization becomes ambiguous and only has meaning inside the head of the person who created the spreadsheet. For example, color may be used to fill cells of a given category and this may seem harmless, but in doing so, you’ve not only created more data, but you’ve created data that have an ambiguous meaning. If you absolutely must use Excel to store data, the only acceptable format you should use as a responsible data steward is a rectangular, flat file. We mean rectangular as only rows and columns in matrix format (e.g., 10 x 5, 12 x 4, etc.), with no “dangling” cells outside of the grid or more than one table in a spreadsheet. We mean flat file as no cell formatting, no embedded formulas, no multiple spreadsheets in the same file, and data entered only as alphanumeric characters. This will ensure that there is no ambiguous information and a machine will have no problem reading your spreadsheet. Your data will be pure and simple and not abused. Broman and Woo (2018) provide an excellent guide that expands on these ideas. Now that that’s out of the way, we can introduce some additional principles for tabular data that will improve how they are used in downstream analysis pipelines. The “tidy” data principles developed by Hadley Wickham (???) are a set of simple rules for storing tabular data that have motivated the development of the wildly popular tidyverse suite of R packages (Wickham et al. 2019). The rules are simple: Each variable must have its own column Each observation must have its own row Each value must have its own cell Graphically, these rules are shown in figure 3.1. Figure 3.1: A representation of the rules for tidy data (from Wickham and Grolemund (2017)). If you’re already using the rectangular, flat file format, adopting the tidy principles should be a breeze. Using these principles may seem unnatural at first because of a difference between what’s easy for entering data vs what makes sense for downstream analyses. The former is what leads to abuse of tables. For examples, dates are often spread across multiple columns, such as having one column for each year of data where the header indicates the year. This convention may be used because it’s easy to add another year of data as an additional column as the data become available. Howver, this is not a tidy format because the date variable occurs across columns. If you wanted to evaluate changes across years, you’d have to reorganize these data in a tidy format. Using a tidy data format also allows you to more easily merge or join data between tables. This is a common task when analyzing data where you have information spread between different tables because 1) it might not make sense to keep the data in a the same table, but 2) the analysis depends on information from both tables. For examples, perhaps you want to evaluate how a measured variable at different locations changes across space. You might have one table that includes station metadata (e.g., site, location) and another table that includes field observations (e.g., site, collection date, field data). Keeping the station metadata in a tidy format in one table makes sense because these data will not change, whereas the keeping field data in another table would make sense because you collect much more information that may be collected at different times. Including station coordinate information in the same table as the field data would create redundant information because you need a value for location for every row you have field data. This is redundant and unnecessary. If you’re using a tidy format, it’s simple to join two tables for analysis. This requires identifying a linking variable or “key” that is a common identifier between tables. In the above example, this would be the station identifier. Other situations may require identifying more complex keys depending on your analysis question. Our question above related to evaluating differences in location between stations, so the station is a logical choice for a key. For all cases, a key is used to resolve a uniquely identifiable value that can be used to link observations. A more involved example is provided in section 5.1. If the important data contribution of your project includes multiple tables, you’ll need to identify an appropriate key. 3.4 Metadata Just as “data” can have different meanings to different people, “metadata” is a loosely defined term that describes one of the most important aspects of data management. Metadata varies from simple text descriptions of a dataset, such as “who”, “what”, “when”, “where”, “why”, and “how”, to more formalized standards with the intent of preparing your data for archival in a long-term repository. Having no metadata is almost a guarantee that your dataset will be orphaned or misused by others, either inadvertently or with willful acknowledgment that the original purpose of the data is unknown and its use may be inappropriate for the task at hand. Metadata are also important for enabling discovery of your data (the F in FAIR). So, when you think of data management, you should think of it as synonymous with metadata curation. At its basic level, metadata is literally defined as “data about data” or “information about information”. A more comprehensive definition is provided by Gilliland (2016): A suite of industry or disciplinary standards as well as additional internal and external documentation and other data necessary for the identification, representation, interoperability, technical management, performance, and use of data contained in an information system We use this definition as a starting point to develop our thinking around best practices for metadata generation and curation. Again, it’s good to emphasize that some metadata is way better than no metadata at all. Just because you are not using industry or disciplinary standards for generating metadata doesn’t mean you’re approach is incorrect. As you get comfortable with the general purpose of metadata and how it’s developed as a description for a dataset, you can build on this knowledge by adopting more formalized standards for developing metadata. At its basic level, think of metadata as a simple text file containing the information about your dataset. This text file provides answers to common questions about the origin of your data so that anyone (or a computer) with zero knowledge about your data can quickly orient themselves as to what the data represents and its purpose. The US Geological Survey provides a useful document on creating Metadata in “plain language” to distill the basic information contained in a metadata file. As indicated above, it provides a workflow for answering the “who”, “what”, “when”, “where”, “why”, and “how” questions for metadata. We provide a brief synopsis of these questions below. You can use this workflow to generate your own metadata. What does the dataset describe? Information here would include very basic details about the dataset including a title, geographic extent, and period of time covered by the data. For geographic extent, this may often include explicit coordinates covering the study area, i.e., the lower left and upper right of a bounding box. Location is useful for indexing your dataset relative to others, if for example, a researcher wanted to find data for all studies in the geographic extent of Tampa Bay. Other useful information about the “what” might include the type of data, e.g., tabular, map, online dashboard, etc. Who produced the dataset? This would be yourself and anyone else who has made a significant contribution to the development of a dataset. People may have differing opinions regarding what defines a “significant” contribution, but as the curator of a dataset, it’s up to you to determine how important it is for including an individual as a contributor. Data are increasingly being used as citable resources and including individuals that were important in its generation ensures proper attribution. For scientific publications, each author is generally expected to have made substantial contributions to the study conception and design, data acquisition or analysis, or interpretation of results. The same would apply to data. If someone has spent hours toiling in the field to collect the data or hours visually scanning a spreadsheet for quality control, include them! Why was the dataset created? Describing why a dataset was created is critically important for developing context. If others want to use your data, they need to know if its appropriate for their needs. Here you would describe the goal or objectives of the research for which the data were collected. It should be clear if there are limitations in your data defined by your goals. For example, you may have collecte field data in a particular time of year to address questions about seasonal changes. Using these data to answer broader temporal questions, such as inter-annual changes, would not be inappropriate and could lead to wrong conclusions if someone using your data were not aware of this limitation. Identifying the “why” of your dataset could also prevent misinterpretation or misuse of the data by non-specialists. Think of it as an insurance policy for your data. How was the dataset created? Here you would describe the methods used to generate the data, e.g., field sampling techniques, laboratory methods, etc. This information is important so others can know if you’ve used proper and accepted methods for generating the data. Citing existing SOPs or methods that are recognized standards in your field would be appropriate. If you are generating a synthesis data product using data from external sources, make sure to document where those data come from and the methods you used for synthesis. Pay attention to documenting the software that was used, including the version numbers. If you have analysis code or script that was used for synthesis, provide a link if possible. How reliable are the data? It’s also very important to document aspects of a dataset that affect reliability. The answers you provide to the above questions can provide context to this reliability, but it’s also imporant to explicitly note instances when the data could be questionable or inappropriate to use. Here you could describe any quality assurance or quality control (QAQC) checks that were used on the data. There are often formalized ways to do so, such as codes or descriptors in tabular data defining QAQC values (e.g., data in range, below detection, sensor out of service, etc.). You will want to clearly describe what each of these codes mean and if they cover the range of conditions possible for your data. Other QAQC procedures, such as how the data were verified for accuracy, can also be described. How can someone get a copy of the dataset? Good metadata always has information on who has the data and how to contact them for requesting access. For archived or publicly available data, this information is more important for who to contact should someone have questions. Information on obtaining a copy of the data should also describe any special software or licensing issues related to accessing the data. Under the I in FAIR, you should strive to make your data as interoperable as possible and not store your data in an obscure format that requires specialized software. If this is unavoidable (e.g., your data are large and it needs to be compressed), describe what needs to be done to access the data. Any licensing or permissions issues on using data should also be described, e.g., is it free for use with or without attribution, are there limitations on its use, etc. The licensing chapter in Wickham and Bryan (2015) is a great place to start to learn more about licensing. Although this chapter relates to code licensing, the same principles could apply to data. 3.4.1 Metadata examples Now that we’ve covered the general concepts of what is included in metadata, we provide some examples of what this looks like in practice. At it’s simplest, metadata can simply be a text file that includes information on the questions above. Below is one such example of metadata that accompanies a dataset that we describe in section 5.2. Figure 3.2: A simple example of metadata illustring the principle that something is better than nothing. Just by looking at the metadata, we can quickly understand some basic information about this dataset. It describes some water quality monitoring data at two buoys near Ft. DeSota in Pinellas Co, Florida. We can see the type of data, how often it’s collected, what equipment was used, the location of the buoys, some contact information should there be questions, and other items that provide context. Although it doesn’t cover all of the questions above, I would be more than happy to use this data since I have some basic knowledge about what’s included. The example in figure 3.2 represents the bare minimum of what should be done to document metadata. This metadata is an excellent example of the principle that some metadata is better than no metadata. So many datasets lack even the simplest information to facilitate their use by others. At its core, metadata should serve the purpose of providing information about information. No matter the level of specificity or metadata standard that was used, all metadata serve this need. However, more formalized approaches to documenting metadata can play an importane role in prepardng a dataset for discovery by others and long-term archiving. The next section provides one example of a metadata standard that could be used for environmental datasets. 3.4.2 The EML standard There are countless standards for metadata that go beyond the simple descriptive text shown above. These standards provide a formalized approach or “schema” to documenting metadata that provides context about a dataset that is also machine readable. The latter component is critical for making sure that all datasets prepared for hosting or archiving at a data repository follow the same standards for documenting metadata. The core pieces of information (who, what, when, where, why, and how) are included, but in a formalized way to allow for rapid searching and queries when the data are stored along with hundreds to thousands of other datasets. One such standard that is useful for environmental data is the Ecological Metadata Language or EML. The EML standard defines a comprehensive vocabulary and a readable XML markup syntax (fancy talk for machine readable) for documenting research data. Importantly, the standard is community maintained and developed for environmental researchers who want to openly share their data. The EML standard is also used by the Knowledge Network for Biocomplexity or KNB, which is an online repository that is federated with a much larger network of online data repositories. The EML metadata file is an XML file that looks something like this: Figure 3.3: A very simple example of an EML file for metadata, shown as an XML file. The file in figure 3.3 might look complicated, but it’s just a way to document the basic components of metadata so that a machine can read them. Regarding the descriptive role of metadata, the above example provides a title for the dataset, a brief description, and who to contact. All the rest is additional information about the standard that was used and basic XML tags to identify parts of the document. The EML provides many more standards to document all other types of metadata information for the questions described above. A specific reason why EML is mentioned here is the availability of additional software tools to help create EML files for your data. In particular, the EML R package provides these tools to streamline metadata creation. Nobody wants to type an XML file by hand, so the EML packages provides a set of functions where a user can input basic metadata information to create the XML file automatically. All you need is a basic understanding of R and metadata to use the EML package for your own needs. More information can be found on the website: https://docs.ropensci.org/EML/ Of course, you can always manually enter your metadata when you submit a dataset to an online repository. Most repositories, KNB included, provide a form entry system for doing so. This may not be the most efficient choice, but is often the preferred for first-timers that may not yet be comfortable using other tools to generate metadata. 3.4.3 Data dictionaries A final note about metadata relates to data dictionaries and what they mean for describing a dataset. A data dictionary can be used for tabular datasets to describe column names and the type of data in each column. This can be incredibly useful for understanding context of a dataset, which is why we include a short description here in the metadata section. However, data dictionaries also have importance for more general best practices for data management. Simple things like how you name a data column can have larger implications for downstream analysis pipelines or interpretability of a dataset. In metadata, a data dictionary can be as simple as the example in figure 3.2 for the parameters that were collected. There we see the column names, units, and formats for time variables. This is invaluable information for others that might want to use your data. Here we provide some general guidelines for developing your own data dictionary. This is all information that can be included in metadata, but it is also useful to consider for data management. Column names Be as descriptive as possible while trying to keep the name as short as possible. Really long names with lots of detail can be just as frustrating as very short names with very little detail. Ideally, the description of data in a column can be included in metadata, but the column name should also be intuitive to point the analyst in the right direction. Try to avoid spaces in column names since some software may interpret that as the start of a new column. Column types Each column includes only one type of data, e.g., numerical measurements, categorical descriptors, or counts of observations. Never, ever mix data types in the same column. If your data are continuous numeric values, try to identify an acceptable range for the values, e.g., are there minimum or maximum values that would indicate the data are out of range? Also make note of the units that were used. For categorical descriptors, identify all possible categories that are acceptable values for the column, e.g., small, medium, or large for a qualitative descriptor of size. For dates, make note of the format, e.g., YYYY-MM-DD. For time, identify the timezone. 3.5 Where do data live? Identifying a location for where you data can be stored long-term can be just as important as using best practices for data curation. Hosting your data in an online repository makes your data findable and accessible by others and also ensures that your data are of sufficient quality to adhere to standards for the repository. There is a staggering variety of online repositories, many of which are domain-specific, and it can be difficult to find the best repository that is suitable to your needs. As with metadata, the same rule applies to online data storage - something is better than nothing. Making your data available in a location that can be accessed by others, including metadata, is much, much better than not sharing your data at all, even if that location is not an “official” data repository. For this purpose, online FTP websites, for example, can be sufficient. Of course, the major drawback of not hosting your data on an official repository is that others can’t easily find the data. You can of course send the link to anyone that’s interested, but this means they need to know the data exist to request the link in the first place. A useful scenario is that you include the location of the data as a supplement link in a published paper or technical report. Hosting data on GitHub is another simple solution to making your data available to a larger community. GitHub is neither a federated repository, nor is it setup specifically for long-term data storage. However, if you already use GitHub and you want to do something rather rather than nothing at all, GitHub can be a useful solution to begin opening your data. GitHub was initially setup as an online platform for software or code version control, so it doesn’t have all the hallmarks of a conventional data repository. GitHub also does not work well with large datasets (e.g., more than 100 Mb). However, it can work well for smaller datasets and offers other amenities that can help you work towards the FAIR principles. For example, the URLs are stable (in the sense that they don’t change), a DOI can be attached to your data (e.g., through Zenodo), the data are publicly accessible if you choose to make them so, and you can include any appropriate supplemental information (i.e., metadata files). GitHub can be especially useful if your data product is a workflow that includes code to create a tool for environmental decision-making. A better, but more involved, solution for opening data is using a federated data repository. These are networks of distributed nodes or individual repositories that collectively use similar standards in archiving data. They address the problem of multiple disconnected archival systems that are difficult to navigate. For example, the KNB repository is one node of the larger DataONE federated network. DataONE includes other repositories that are domain-, industry-, or regionally-specific that collectively fall under a more generic category of environmental or earth sciences data. All nodes in the larger DataONE network can be easily navigated and have full infrastructure support from DataONE. The main advantage of hosting your data in a federated repository is that it will be truly discoverable - it can be found online through standard search queries. No prior knowledge is needed about the data for someone to find the information. For example, perhaps someone is interested in finding datasets within a specific geographic location. They can search the federated network with these criteria and your dataset will be returned if it’s within the boundaries. Your metadata includes that information as a queryable attribute. Another advantage is that your data should live on in perpetuity, so long as the internet exists. As mentioned above, GitHub can be a location to store data for open access, however, there is no guarantee that GitHub will always be available as an online service. Federated repositories take great measures to ensure the long-term viability of their resources, including multiple distributed backups in different locations and interoperability of datasets across platforms. You receive those benefits as a guarantee when your data are hosted on these services. References "],["workflow.html", "Section 4 Data Management Workflow 4.1 The TBEP approach 4.2 How can you manage data?", " Section 4 Data Management Workflow This section describes our approach to managing data internally, to help describe what we do and why we do it. This is context for section 4.2 that provides a road map for opening internal or external datasets. 4.1 The TBEP approach 4.1.1 Our philosophy The TBEP data QMP (E.T. Sherwood, G. Raulerson, M. Beck, M. Burke 2020): what, why, how The Open Science cake: what it is, how we do we implement it, and what does it mean for data management, figure 4.1 Figure 4.1: The open science cake showing the connection between research, environmental decisions, and the public. Applied science, not implied science General workflow - source to product, figure 4.2 Figure 4.2: The TBEP open science workflow connecting source data to decision-support products. 4.1.2 Tools we use R/RStudio IDE workflow The tbeptools package as central component How does the package facilitate the above? GitHub as a collaborative tool and quasi-archive version control collaborative tool to work together - issues, pull requests DOI through Zenodo CI/CD, Automation with GitHub Actions and badges GitHub linkage to TBEP website 4.2 How can you manage data? Ten simple rules for creating a good data management plan: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004525 Section is written as a road map for developing a data product, there will be steps/checkboxes/forms, roughly following figure 4.3 Figure 4.3: A hypothetical and generalized timeline for managing data associated with a project. Modularity is key to reproducibility, it is independent of where you’re at in the project Setup some kind of flow chart (if this, then that) 4.2.1 I’m at the beginning of my project What type of project am I working on? What types of products am I expecting? Which datasets are important? Guidance for determining which datasets are important are expressed in great detail in section 3. How do I want to make the data accessible? What QA protocols should be established? 4.2.2 I’m somewhere in the middle of my project Have I collected data? Are my data in tidy format (if tabular)? Have I been documenting metadata? 4.2.3 I’m at the end of my project Time for damage control 4.2.4 Metadata workflow USGS resources https://www.usgs.gov/products/data-and-tools/data-management Metadata questionnaire https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/atoms/files/MetadataQuestionnaire_508compliant.pdf Data dictionaries https://www.usgs.gov/products/data-and-tools/data-management/data-dictionaries 4.2.5 Let’s get it online! How to of where do you put your data References "],["cases.html", "Section 5 Case Studies 5.1 TBERF oyster restoration project 5.2 DeSoto/RESTORE project 5.3 Red Tide Twitter repo", " Section 5 Case Studies Demonstrate the workflow, identify questions that were asked initially and what does done to answer them 5.1 TBERF oyster restoration project Initial questions - Are we formatting our data correctly? Example of mid-project data curation Emphasis on data dictionary, normalized tables 5.2 DeSoto/RESTORE project Initial questions - how can we fulfill RESTORE requirements for data delivery based on a general grant requirement? Example of continuous stream data Emphasis on CI/CD checks and web products 5.3 Red Tide Twitter repo Initial questions - What are the most relevant products from this project and how can we make them accessible? Example of specific data product with linkage to technical and primary lit publication Emphasis on creating a GitHub repo for archive of lexicon and source data, DOI "],["final.html", "Section 6 Final Words", " Section 6 Final Words emphasis on “something is better than nothing”, fully open is ideal but difficult to achieve, e.g., “good enough practice” (Wilson et al. 2017) Just remember FAIR evolving tools trying to be both a domain expert and data expert will spread you thin (Mons 2018, 27, 36), look to the helpers/community (Figure 6.1) Join the community of practice by getting involved in the Open Science Subcommittee Figure 6.1: Look to the helpers and your open science community! Artwork by @allison_horst. References "],["appendices.html", "Section 7 Appendices 7.1 List of resources 7.2 Data types 7.3 Definitions 7.4 Metadata templates", " Section 7 Appendices 7.1 List of resources From EDI https://environmentaldatainitiative.org/dm-resources/ 7.2 Data types Field data Survey forms Tabular* Database of tables Database Synthesis New Model Actual model Model results Dashboard 7.3 Definitions Dashboard Data Aggregation vs. synthesis Model Tabular Database Flat file Tidy data 7.4 Metadata templates General - Who, what, when, where, why? Specific - XML, EML, etc. "],["references.html", "References", " References "]]
