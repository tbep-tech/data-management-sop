[["index.html", "Data Management SOP for the Tampa Bay Estuary Program Section 1 Overview 1.1 Contributing to this document 1.2 About", " Data Management SOP for the Tampa Bay Estuary Program Marcus W. Beck, Gary E. Raulerson 2021-01-19 Section 1 Overview Welcome to the Tampa Bay Estuary Program Data Management SOP. This document describes our philosophy for managing data used by our program and serves as motivation for our external partners to become stewards of their own data. Working together, we can improve how data are used to support the continued protection and restoration of Tampa Bay. 1.1 Contributing to this document Using an open science ethos, we strongly encourage community collaboration in how this document evolves. This means anybody can contribute directly to content in this document. Please follow the guidelines in section 1.1 to learn how to contribute and improve this SOP. This SOP was created using bookdown, which is an approach to creating long-form documents with RMarkdown. The source code is available on the TBEP GitHub group web page: https://github.com/tbep-tech/data-management-sop. Each section is a plain .Rmd text file that can be edited or commented to provide feedback on content. There are several ways you can contribute to or edit this document. Before you choose your editing option, you should be comfortable with Git/GitHub basics and have some working knowledge of RMarkdown files (but see 1.1.4). The first step is to make sure you have a GitHub account so you can edit the files. Jenny Bryan’s Happpy Git and GitHub for the useR is an excellent resource to get started with version control. R Markdown: The Definitive Guide is a great resource for learning RMarkdown (also see the cheatsheet). 1.1.1 Option 1 Requires: GitHub account, write access to the source code repository Each section can be edited directly by selecting the edit button at the top of the page. Clicking on the edit button will take you to GitHub, where you will see an edit page like this: Each edit page is specific to the section where you’ve selected the edit button, e.g., if you click the edit button for section 2, you’ll be sent to the edit page for the .Rmd file for section 2. Feel free to make any changes on the .Rmd file. When you’re done, scroll to the bottom and “commit” your changes. This simply means you write a few words describing the edits you’ve made. Be as succinct as possible. When you’re done, hit the green “Commit changes” button. 1.1.2 Option 2 Requires: Github account Follow the above steps in 1.1.1 by navigating to a section you’d like to edit on this website and selecting the edit button. If you don’t have write access to the repository, you will see something like this: This simply means that you need to create your own copy to edit. You can fork your own copy to your personal account and make your edits there. Once editing is done, you can submit a pull request to the original repository with your proposed changes. Not sure what this means? Check out this chapter here: https://happygitwithr.com/fork-and-clone.html 1.1.3 Option 3 Requires: GitHub account If none of the above sounds appealing, you can always post any suggestions or edits as an issue under the issues tab of the repository. When you create a new issue by clicking the giant green “New issue” button, you’ll see something like this: Give your issue a short but informative title (e.g., “suggests edits to section 2”). Under the “Write” tab, explain what edits or changes you’d like to see. Feel free to select a member of the TBEP staff to assign the issue using the menu on the right. The issues descriptions support Markdown syntax, so get creative in your descriptions (i.e., make lists, link to documents, etc., see the cheatsheet). In general, one issue should cover only one suggested change to the document. However, multiple text edits to the same document can be submitted to the same issue so long as they cover similar topics, e.g., one issue for several suggested edits to one section. 1.1.4 Option 4 Requires: Email Just email me any changes you’d like to see! 1.2 About The Tampa Bay Estuary Program is one of 28 National Estuary Programs designated by Congress to restore and protect “estuaries of national significance.” Administered by the U.S Environmental Protection Agency under the Clean Water Act, each program must develop a science-based plan using community input to protect and enhance the natural resources of its respective estuary and surrounding watershed. The Comprehensive Conservation and Management Plan (CCMP, updated in 2017) presents 39 actions to sustain progress in bay restoration through the year 2027. To address the actions in our CCMP, our 2021-2025 Strategic Plan provides a framework to guide decisions about how to align personnel and financial resources with the Program’s mission in ways that maximize our impact on Tampa Bay recovery. A cornerstone strategy of this plan is the use of open science principles and methods to allow the TBEP to be the primary source of trusted, unbiased, and actionable science for the Tampa Bay Estuary. This document is a reflection of these strategies. Please visit our website for additional information about our program: https://www.tbep.org This book is licensed under a Creative Commons Attribution 4.0 International License. This version of the book was built automatically with GitHub Actions on 2021-01-19. "],["background.html", "Section 2 Background 2.1 Importance of data 2.2 Why we need to effectively manage data 2.3 Open Science 2.4 The TBEP philosophy 2.5 Goals and objectives of this document", " Section 2 Background 2.1 Importance of data Informed decisions about how we manage behaviors and actions that affect the environment cannot be made without data. As a fundamental part of the scientific method, data provide the raw information to support hypotheses that inform our understanding of natural processes. Environmental research develops this understanding to support informed decisions for managing natural resources and data are the foundation for research. As methods for managing environmental resources continue to evolve, so does our understanding of data beyond its conventional role in an academic setting. When we discuss “data” we often describe a very general term that has different meanings for different people. In it’s simplest form, data can be tabular information for the results of an experimental analysis or field survey. For environmental managers, data can mean the long-term record of routinely collected and updated monitoring information to assess status and trends of a natural resource. Even further, data can be highly aggregated and novel products created through complex meta-analyses of independent datasets. In all cases, the need to describe a dataset, its purpose and origin, and to identify a permanent and long-term home for data are critical to ensure forward progress in both conventional research and how research informs environmental management. This document describes one approach to address data management needs for the long-term restoration and protection of natural resources in Tampa Bay and its watershed. We are inclusive of multiple definitions of data from simple spreadsheets to more complicated workflows for generating reporting products that support environmental decisions. This broad net is purposeful to account for both the variety of data we use as a program and the diversity of partner agencies we depend on to achieve our mission. The management of environmental resources combines a healthy mix of conventional science with consensus driven decisions, often with strong regulatory overtones. The foundation for all of these processes requires a robust approach for working with data that promotes trust and validity in the environmental decision and to ensure the science continues to progress without reinventing the wheel. 2.2 Why we need to effectively manage data There are many reasons why data may not be effectively managed, chief among which is that it can be tedious and unglamorous work that is often an afterthought. The value of investing a little time in managing a dataset can have long term benefits, but these values may not be obvious or realized in the short-term. This leads to many “orphaned” datasets and data products that at one point may have had a known origin and purpose, but over time this information is lost because it is not formally documented or in a location that is findable and accessible by others. A classic graphic in Michener et al. (Michener et al. 1997) demonstrates how knowledge of a data product is lost over time (Figure 2.1). As a research team publishes a paper, the information content of the data and metadata is at an all time high because the ideas and concepts have been intensely studied and evaluated in the months leading up to publication. After publication, researchers move on to other projects or responsibilities and specific details about the intial project are lost rapidly as attention is focused elsewhere. At longer time scales, other factors can contribute to the erosion of information content, including career changes through retirement or staff turnover or accidental loss of information (laptop takes a bath, lab fires, etc.). Figure 2.1: Loss of information over time in the absence of data management (Michener et al. 1997) The final step in Figure 2.1 leading to absolute and complete loss of information on a data product is death of an investigator. Although a bit morbid, this is a very real and preventable problem in the process of discovery that can you lead down the frustrating path of reconstructing a data product’s origin through scouring historical records that have little to no descriptive information. As a remedy, many research teams have adopted the “bus factor” term as motivation to prevent this problem. The bus factor is an informal measurement of the relative risk associated with loss of information if an investigator was to, hypothetically, be hit by a bus (alternatively, the “lottery factor” describes departure of an individual if they were to win the lottery). Data sets or workflows that have a high bus factor are at risk of being orphaned with the untimely departure of a team member. The costs of not effectively managing your data can vary, but each is a byproduct of neglecting an investment in data management. In fact, you can probably recall several past instances when poor data management has come back to haunt you. Here are a few examples, some from my own experience and some from others: A collaborator calls you on the phone asking about a historical dataset from an old report. You spend several hours tracking down this information because you don’t know where it is. The data you eventually find and provide to your collaborator has no documentation and they don’t know how to use it or use it inappropriately. You receive a deliverable from a project partner that was stipulated in a scope of work. This deliverable comes in multiple formats with no reproducible workflow to recreate the datasets. You are unable to verify the information, eroding your faith in the final product and making it impossible to update the results in the future. An annual reporting product requires using new data each year. The staff member in charge of this report spends several days gathering the new data and combining it with the historical data. Other projects are on hold until this report is updated. Stakeholders using this report to make decisions do not trust or misunderstand the product because the steps for its creation are opaque. A more general problem with poor data management is stifled creativity. The use of other people’s data and services (i.e., “OPEDAS”; Mons 2018) to generate novel research or data products is increasingly common, particularly in the last twenty plus years with the advance of internet communications. Entire disciplines and new methods have been developed around this idea (e.g., meta-analysis; Carpenter et al. 2009; Lortie 2014). The generation of new data that have an incomplete history or that lack metadata documentation is a disservice to both the researcher that created the data and the larger scientific community that could benefit from using this information. As a result, scientific progress will not continue as rapidly as it could if data products are discoverable and openly available. Poor data management can also lead to peculiar or entrenched workflows that may work for an individual but are not scalable or translatable for use by others. Many of us, myself included, have our own preferences for how we manage our data that simply “works for us”, either because we learned out of necessity because the work had to be done or we’ve just been doing it a certain way for so long that it now seems normal despite being inefficient or prone to error. In extreme cases, this can lead to workflows that may seem legitimate but are problematic because they lack a formality or standards that are common in other disciplines. Mons (2018) describes “professorware” as one type of workflow for handling or generating data that address a novel intellectual challenge, which are important in research or discovery, but are not scalable or sustainable in the long run. Think of a pet project where you’ve written some code to achieve a certain task. It might be clunky, but you’re proud of it because it gets the job done on your computer and saves you from having to do a task by hand. These workflows can often masquerade as novel “software packages” that do great things, which they often and can do, but they lack support because they’re not often developed using community standards or best practices for long-term use or scalability. This is especially problematic when these workflows are intentionally or unintentionally embedded into larger data management systems. If one piece of the system lacks provenance or support, it puts the larger data management system at risk. In summary, poor data management practices can lead to the following: Less collaboration in the research community Increased siloing among management institutions Less creative approaches to managing environmental resources Inefficient and error prone workflows that are neither scalable nor sustainable 2.3 Open Science The open science approach provides a philosophy and set of tools to help address the costs of poor data management. Before we proceed, we need to make a distinction between the broader concept of open science and open data as one component of the former. Many of the guidelines and examples in this SOP fall broadly under the open science umbrella (or cake as you’ll see later), but it’s important to understand how data management includes a set of tools that are part of, but not exhaustive of, the entire open science toolbox. Conversely, many broadly applicable open science tools that can be applied to other management scenarios can also benefit data management. An example may be helpful. A key component of data management that leads to more open sharing is metadata. Although metadata can be created in an open environment and is often created for the purpose of facilitating openness, it can also be created completely in isolation with a closed workflow. So, when we talk about metadata, the assumption is that its creation is to promote sharing and transparency for open data, whereas metadata by themselves are only so useful in how they can facilitate the latter. On the other hand, broader open science principles that support a culture of sharing can also have value for research workflows that generally have nothing to do with data. For example, the “public school of thought” for open science focuses on making science more accessible to the general public, e.g., through citizen science initiatives or science blogging (Fecher and Friesike 2014). Although this approach doesn’t deal explicitly with best practices for data management, this mentality certainly has benefit for creating a culture that appreciates and learns from science, which logically leads to discussions on the importance of data. For these reasons, this document covers many topics that may fall squarely under the realm of data management, while at other times advocating for more general open science principles with the intent of supporting a culture of better data management. 2.4 The TBEP philosophy The Tampa Bay Estuary Program (TBEP) is one of 28 National Estuary Programs designated by Congress to restore and protect “estuaries of national significance.” Many of these estuaries are heavily urban (i.e., having economic, recreational, cultural importance) and have had historical or ongoing issues contributing to poor environmental quality. The recovery of Tampa Bay is an exceptional story of an urban estuary that demonstrates the value of the NEP approach to restoring and protecting environmental resources. Through a coordinated regional effort of environmental professionals, utility operators, and local politicians, nutrient loads to the Bay have been reduced by ~2⁄3 from 1970s levels and seagrasses have recovered to a 1950s benchmark extent (Greening et al. 2014; Sherwood et al. 2017). What makes this story even more remarkable is that the human population in the Tampa Bay watershed continues to increase while nutrient loads into the Bay remain low. The TBEP is a key facilitator among the many local partners that have an interest in the region’s natural resources. Our facilitation is guided by several documents, including an Interlocal Agreement with our partners, a Comprehensive Conservation and Management Plan, and a Strategic Plan. In simple terms, these documents respectively describe who we work with, what we need to accomplish, and how it can be accomplished. Open science and data management have everything to do with how we facilitate Bay management. Our recent update to the Strategic Plan specifically speaks to our use of open science as 1) a general direction for how we accomplish our work to achieve the desired future state of Tampa Bay and 2) as a unique value proposition that TBEP offers within its sphere of influence. We articulate the use of open science at TBEP as a cornerstone strategy: Be the primary source of trusted, unbiased, and actionable science for the Tampa Bay estuary, recognizing that open science principles will serve the Program’s core values. As a program with only seven employees, we realize our success depends on the work of our many partners. We can use open science principles internally, but we can have a much greater impact if who we work with understands the value of open science and actively works towards adopting its principles in their own workflows. We are actively supporting our partners through this journey through an open science subcommittee that has a goal of developing a community of practice that works and learns together to navigate the open science landscape. Our roles and responsibilities document explains how we are accomplishing this goal. Our program rests on a strong foundation of research that guides decision-making for Tampa Bay covering nearly three decades of science and collaboration. Although great strides have been made, we can do better as an organization and this starts with better data management practices guided by open science principles. The details of this approach and why we’ve adopted open science as our own are explained fully in section 4.1.1. 2.5 Goals and objectives of this document The overarching goal of this document is to achieve the following: Motivate internal staff and external partners to become stewards of their data by demonstrating the value of open data practices and providing a road map to achieving this goal. Each section of the SOP address a critical topic or provides a roadmap that collectively helps us work towards achieving this goal. The sections are as follows: Section 3: An overview of general and specific topics that are useful to understand for data management Section 4: An explanation of how TBEP manages data and a roadmap to developing your own workflows Section 5: Examples describing specific projects and why/how data management practices were applied to each. Section 6: Parting thoughts and words of wisdom to help you continue on your open science and data management journey Section 7: A list of resources for continue learning or additional content supporting earlier sections The TBEP has also developed a data Quality Management Plan (QMP; E.T. Sherwood, G. Raulerson, M. Beck, M. Burke 2020). This SOP and the QMP can be viewed as partner documents that are complementary to each other, but are developed to meet different needs. The goal of this SOP is described above. The goal of the QMP is to ensure the data used by TBEP for decision-making has known and documented quality and is being used appropriately. In other words, the QMP establishes an internal process for ensuring data quality standards are in conformance with federal requirements, whereas the SOP document is a generalized introduction and how-to approach for data management that can help us achieve goals of the QMP. Identifying what this SOP is and what it is not can also help us set expectations for what this document can achieve. As noted above, the TBEP is a relatively small organization with hands in many projects supported or managed primarily by our partners. It would be inappropriate and impossible to describe a detailed step-by-step SOP that could apply to every project. So, the approach and workflows we describe are meant to be generalizable to many types of projects. Any specificity that is described relates to how to use tools that have broad applicability, e.g., developing a GitHub workflow or describing general characteristics of metadata that could apply to many data types. This distinguishes our SOP from others that may apply rigorous standards to one particular problem. To summarize, this document is: An explanation of the TBEP approach to data management, including our philosophy and the existing tools we have developed A generalized cookbook describing how to manage datasets in an open science framework, including considerations before, during, and after a project This document is not: A definitive overview of best practices for data management, there are other resources (see section 7) that cover these topics in more detail A comprehensive list of available online services for opening data, although we certainly lean towards specific platforms that we find useful Finally, the intended audience for this SOP is TBEP internal staff and our external partners. In both cases, the text is written to target technical staff, although the concepts and principles that are advocated should also appeal to managers or higher administrative staff. These individuals are in a position to foster better practices for data management by creating space and time for technical staff to adopt these new workflows. Understanding the importance of the tools is important, but sufficient space must be available for these skillsets to grow through a shared community of practice. Over time, the return on investment in creating a space for these skillsets to develop will be realized. References "],["keys.html", "Section 3 Key Concepts and Principles 3.1 General concepts 3.2 Specific concepts", " Section 3 Key Concepts and Principles Before we get started, we need to discuss some basic ideas around data and their management. Understanding these concepts and why they’re important will facilitate the development and curation of open data for both yourself and others to use. We start first with general concepts and then dive into some detailed concepts. The detailed concepts may seem daunting, but they are critical in supporting your journey in managing your own data. 3.1 General concepts 3.1.1 Data types and identifying important contributions We briefly introduced a general concept of data in section 2.1. Here and throughout, we use the term “data” to describe a variety of products either directly supporting decision-making processes or used for research to support the former. Data can be generated to support or refute hypotheses in research, whereas research can also produce data products that support environmental management. The end game in all of these processes is Understanding that data can be present at any stage in research and/or decisions that support environmental management. Individuals may generally use the term “data” to describe products at any point in this workflow. Understanding the diferent ways we talk about data will allow you to more carefully identify your data management needs. Identifying the types of data that are important to support decision-making is the first task in developing a data management workflow. Any research project could produce countless datasets and it may be challenging to understand which datasets are important or are merely intermediate steps in a larger process. Ask these questions to help you identify which datasets are important to your project. What is the most important and tangible contribution of this project? Who is going to benefit from the results of this project? How can I use data management practices to make the use of these data “easier” for decision-making? Answers to these questions can help you identify important data products that needs a formal data management workflow. However, coming to a single answer is the exception not the norm and a typical answer usually is “it depends”. Also realize that you may be the direct beneficiary of a particular research project - documenting and using proper data management workflows will save you from headache in the future. Evaluating these questions at different steps throughout a project can help you identify the valuable contributions. In a perfect world where we have endless time and resources, and not to mention interest, to dedicate to data management, we would track and document the provenance of every single dataset used by a research project. Of course, this is impractical, nor do we need to curate every piece of data. You will need to identify the most important contribution of a project among alternatives based on your answers to the above questions. Here are a couple scenarios that can help in this process. I am collecting field data and/or running experiments in a laboratory. The field or experimental data are obvious candidates for developing a data management workflow, yet it is rarely one dataset that is produced. Working with these data continuously throughout a project will benefit from developing a data dictionary and understanding linking keys between different data tables. If you don’t want or need to archive all the datasets you’ve used or created, identify a master dataset that provides the main results for your study. I am using data from an external source as primary or secondary information to support analysis or generate a reporting product A derived dataset may be the most important contribution of this project. This dataset includes multiple combinations of input datasets from external sources. It is important to document the steps that were used to develop this dataset, including the raw sources of information and where they can be accessed. Documentation could be a general description (less desirable) to reproducible source code (most desirable) to create the derived dataset. The most important contribution may be the workflow or the derived dataset, depending on “who” can benefit most from this project. I am producing a model to support scenario exploration or understanding of natural processes Tracking data provenance of a modelling project is a challenging task simply because a “model” does not conform to the conventional understanding of data. As noted above, we describe data as anything that can support decision-making in environmental management. Models are commonly used for this task, yet understanding of their information content over time often rests with one individual, giving that modeller a very high bus factor. There are practical limitations for fully tracking a model as a data product (e.g., computational limits, time requirements, required knowledge of its working components), but there are certainly derived datasets from models that can benefit from data management. In particular, model results, parameters, or source code are all prime candidates for data management, depending on the audience. I am developing a decision-support tool Related to the challenges of data management for modelling, so-called “decision-support” tools are increasingly used as a front-end for decision-makers to access relevant information from a research project or intensive data collection effort. Online interactive dashboards have proliferated tremendously in the last ten years to meet this need. These tools can be useful in the right hands, yet there is no community standard for how to treat these products as data to track their origin and metadata. In this case, documenting the workflow, source code, and requisite datasets for powering the dashboard may be the most important contributions. 3.1.2 The FAIR principles The previous section presented several questions to ask yourself that can aid in identifying important contributions of a research project as a focus for data management. In all cases, once that important contribution is identified, community standards or best practices for that dataset or product should be used to ensure the intended audience can find, access, use, and replicate the data. The FAIR principles (Wilkinson et al. 2016) provide some general guidelines to follow for ensuring the openness of a data product. The FAIR acronym is described as follows: Findable: The data have a globally unique and persistent identifier, including use of “rich” metadata Accessible: Once found, the data can be retrieved using standardized communications protocols that are open, free, and universally implementable Iinteroperable: The ability of data or tools from non-cooperating resources to integrate or work together with minimal effort Reusable: If the above are achieved, the data and metadata are described in a way that they can be replicated and/or combined in different settings. What this means simply is that 1) each dataset has a name that doesn’t change and can be found with minimal effort using that name, 2) once it’s found you can actually get your hands on it (e.g., not behind a paywall), 3) once you have it, you can use readily available tools to work with the data (e.g., not using proprietary software), and 4) you can actually apply the data for your own needs becuase it has sufficient context, including its reproduction, given the the first three principles are met. The FAIR principles invoke several concepts that will be described in detail later, but we describe some here as a gentle introduction. The term “globally unique and persistent identifier” is a mouthful that simply means the dataset has a name assigned to itself that is not assigned to any other dataset (globally unique) and it’s permanent (persistent). This doesn’t mean a descriptive or literal name, such as you would assign to a file on your own computer, rather it means a computer-generated identifier created using a known standard. One such example is a DOI, or digital object identifier. These are commonly assigned to publications as a static web address (persistent) and are increasingly being used as identifiers for datasets. Findable and accessible also imply the data have a home with an address. The latter describes the unique identifier, whereas the home itself is permanent location as a requirement for accessibility. There are several options for where data can live long-term and theoretically forever so long as the internet exists. There are literally thousands of repositories online that can be used for data archival and the answer to which repository you should use is almost always going to be “it depends”. We provide some examples in section 4.1.2 as one option used by TBEP. The FAIR principles are not rigorous standards, rather they establish some general questions you can ask of a dataset to make sure you’ve done your due diligence in achieving openness. Further, because they are not rigorously defined, different organizations may interpret the principles differently and it’s important to realize that your understanding of the principles may differ from others. For example, individuals may define “reusable” in different ways that can affect the level of detail provided in the metadata. These principles are presented here as a reminder to think about them often, especially during the beginning of a project, and how they can be applied in opening the most important contribution of your project. 3.1.3 Metadata What is metadata and why do we need it? Best practices for QA 3.2 Specific concepts Types of data products (e.g., raw data, models, synthesized/derived data, etc.) or types of data (flat file, spatial, disparate) Tabular data an overview of tidy data, can a machine read it? The wrong approach Basic database principles logical extension of tidy data normalized tables (including discussion of key variables), what are unique ids (e.g., tberf oyster, how did I make the unique id?), facilitate standard DB queries Metadata principles Why? Supports the F in FAIR, also supports use by others Minimum requirements Formal standards data dictionaries, naming conventions Where do data live long-term, what’s a doi, considering a data paper, federated repository, etc. The A in FAIR GitHub repository Stable URL Official repository References "],["workflow.html", "Section 4 Data Management Workflow 4.1 The TBEP approach 4.2 How can you manage data?", " Section 4 Data Management Workflow This section describes our approach to managing data internally, to help describe what we do and why we do it. This is context for section 4.2 that provides a road map for opening internal or external datasets. 4.1 The TBEP approach 4.1.1 Our philosophy The TBEP data QMP (E.T. Sherwood, G. Raulerson, M. Beck, M. Burke 2020): what, why, how The Open Science cake: what it is, how we do we implement it, and what does it mean for data management, figure 4.1 Figure 4.1: The open science cake showing the connection between research, environmental decisions, and the public. Applied science, not implied science General workflow - source to product, figure 4.2 Figure 4.2: The TBEP open science workflow connecting source data to decision-support products. 4.1.2 Tools we use R/RStudio IDE workflow The tbeptools package as central component How does the package facilitate the above? GitHub as a collaborative tool and quasi-archive version control collaborative tool to work together - issues, pull requests DOI through Zenodo CI/CD, Automation with GitHub Actions and badges GitHub linkage to TBEP website 4.2 How can you manage data? Section is written as a road map for developing a data product, there will be steps/checkboxes/forms, roughly following figure 4.3 Figure 4.3: A hypothetical and generalized timeline for managing data associated with a project. Modularity is key to reproducibility, it is independent of where you’re at in the project Setup some kind of flow chart (if this, then that) 4.2.1 I’m at the beginning of my project What type of project am I working on? What types of products am I expecting? How do I want to make the data accessible? What QA protocols should be established? 4.2.2 I’m somewhere in the middle of my project Have I collected data? Are my data in tidy format (if tabular)? Have I been documenting metadata? 4.2.3 I’m at the end of my project Time for damage control 4.2.4 Metadata workflow USGS resources https://www.usgs.gov/products/data-and-tools/data-management Metadata questionnaire https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/atoms/files/MetadataQuestionnaire_508compliant.pdf Data dictionaries https://www.usgs.gov/products/data-and-tools/data-management/data-dictionaries 4.2.5 Let’s get it online! How to of where do you put your data References "],["cases.html", "Section 5 Case Studies 5.1 TBERF oyster restoration project 5.2 DeSoto/RESTORE project 5.3 Red Tide Twitter repo", " Section 5 Case Studies Demonstrate the workflow, identify questions that were asked initially and what does done to answer them 5.1 TBERF oyster restoration project Initial questions - Are we formatting our data correctly? Example of mid-project data curation Emphasis on data dictionary, normalized tables 5.2 DeSoto/RESTORE project Initial questions - how can we fulfill RESTORE requirements for data delivery based on a general grant requirement? Example of continuous stream data Emphasis on CI/CD checks and web products 5.3 Red Tide Twitter repo Initial questions - What are the most relevant products from this project and how can we make them accessible? Example of specific data product with linkage to technical and primary lit publication Emphasis on creating a GitHub repo for archive of lexicon and source data, DOI "],["final.html", "Section 6 Final Words", " Section 6 Final Words emphasis on “something is better than nothing”, fully open is ideal but difficult to achieve, e.g., “good enough practice” (Wilson et al. 2017) Just remember FAIR evolving tools trying to be both a domain expert and data expert will spread you thin (Mons 2018, 27, 36), look to the helpers/community (Figure 6.1) Figure 6.1: Look to the helpers and your open science community! Artwork by @allison_horst. References "],["appendices.html", "Section 7 Appendices 7.1 List of resources 7.2 Data types 7.3 Definitions 7.4 Metadata templates", " Section 7 Appendices 7.1 List of resources 7.2 Data types Field data Survey forms Tabular* Database of tables Database Synthesis New Model Actual model Model results Dashboard 7.3 Definitions Dashboard Data Aggregation vs. synthesis Model Tabular Database Flat file Tidy data 7.4 Metadata templates General - Who, what, when, where, why? Specific - XML, EML, etc. "],["references.html", "References", " References "]]
